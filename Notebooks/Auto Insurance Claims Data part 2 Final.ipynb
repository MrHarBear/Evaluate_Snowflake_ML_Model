{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d79cba-e201-4969-8d76-ea883e04cea1",
   "metadata": {
    "name": "Markdown1",
    "collapsed": false
   },
   "source": "## 1. Import packages and data"
  },
  {
   "cell_type": "code",
   "id": "deeceff7-f7c3-4a9a-a2ce-1f6c7c2181c2",
   "metadata": {
    "language": "python",
    "name": "Import_Packages",
    "collapsed": false
   },
   "outputs": [],
   "source": "#Import snowflake ML Packages\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode\n)\nfrom snowflake.snowpark import Window, Session\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import *\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import *\nfrom snowflake.ml.registry import Registry\n\n# Import other python packages\nimport streamlit as st\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport streamlit as st\nfrom operator import itemgetter\nimport json\n\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdf37afd-84b3-4334-972f-98d2125570aa",
   "metadata": {
    "language": "python",
    "name": "Session_Check",
    "collapsed": false
   },
   "outputs": [],
   "source": "snowflake_environment = session.sql('select current_user(), current_version()').collect()\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.ml import version\n\ndatabase = session.get_current_database()\nschema = session.get_current_schema()\n# Current Environment Details\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(database))\nprint('Schema                      : {}'.format(schema))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(VERSION[0],VERSION[1],VERSION[2]))\nprint('Snowflake ML version        : {}.{}.{}'.format(version.VERSION[0],version.VERSION[2],version.VERSION[4]))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41665d14-5a31-4608-bee3-7a63edcd6f3e",
   "metadata": {
    "language": "python",
    "name": "Import_Claim_Data",
    "collapsed": false
   },
   "outputs": [],
   "source": "claim_data = session.read.table(\"CONTAINER_RUNTIME_LAB.DATA.CLAIM_DATA\")\nclaim_data_new = session.read.table(\"CONTAINER_RUNTIME_LAB.DATA.CLAIM_DATA_NEW\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9808f377-f15a-499a-ba3d-7f1c92031f1f",
   "metadata": {
    "language": "python",
    "name": "Combine_Claim",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Add the SOURCE column to each DataFrame\nclaim_data = claim_data.with_column('SOURCE', lit('ORIGINAL'))\nclaim_data_new = claim_data_new.with_column('SOURCE', lit('NEW'))\n\n# Get the list of column names for both DataFrames\ncolumns_claim_data = [field.name for field in claim_data.schema.fields]\ncolumns_claim_data_new = [field.name for field in claim_data_new.schema.fields]\n\n# Create a set of all column names from both DataFrames\nall_columns = set(columns_claim_data).union(set(columns_claim_data_new))\n\n# Convert the set of all columns to a list and sort it to have consistent column order\nall_columns_sorted = sorted(all_columns)\n\n# Reorder columns in both DataFrames\nclaim_data = claim_data.select([col(c) for c in all_columns_sorted])\nclaim_data_new = claim_data_new.select([col(c) for c in all_columns_sorted])\n\n# Now, perform the union\nclaim_data_combined = claim_data.union_all(claim_data_new)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "fs_define",
    "collapsed": false
   },
   "source": "fs = FeatureStore(\n    session=session, \n    database=database,\n    name='DATA',\n    default_warehouse='CONTAINER_RUNTIME_WH',\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Import_fs_Data",
    "collapsed": false
   },
   "source": "customer_fv = fs.get_feature_view(\n    name = 'customer_data',\n    version = 'V1'\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ade5bb60-3ccc-498a-9455-99bdee58a2bd",
   "metadata": {
    "language": "python",
    "name": "Claim_Data_Cleanse",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "claim_data_combined = claim_data_combined.replace('?', None)\ncounter = claim_data_combined.count()\nprint(f\"New claim_data_combined count:  {counter}\")\nclaim_data_combined.filter(col(\"POLICE_REPORT_AVAILABLE\").is_null()).show(10)\n# Calculate the mode of the 'POLICE_REPORT_AVAILABLE' column\nmode_value = claim_data_combined.select(mode(col(\"POLICE_REPORT_AVAILABLE\"))).collect()[0][0]\nprint(f\"Fill NULL value in POLICY_REPORT_AVAIALLBE to the mode: {mode_value}\")\n# Fill NULL values with the mode\nclaim_data_combined = claim_data_combined.with_column(\"POLICE_REPORT_AVAILABLE\", \n    when(col(\"POLICE_REPORT_AVAILABLE\").is_null(), mode_value)\n    .otherwise(col(\"POLICE_REPORT_AVAILABLE\")))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Generate_Full_Data",
    "collapsed": false
   },
   "source": "data_combined = fs.generate_dataset(\n    name=\"fraud_classification_demo\",\n    #version='v21',\n    spine_df=claim_data_combined,\n    features=[customer_fv],\n    spine_label_cols = [\"FRAUD_REPORTED\"]\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "09e76e65-ebf7-44a3-a5fc-5643c4a4394a",
   "metadata": {
    "language": "python",
    "name": "Drop_Fields",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Understand the policy duration from the policy start date to the indicent date\nfrom snowflake.snowpark.functions import col\n\ndata_combined_df = data_combined.read.to_snowpark_dataframe().with_column(\"POLICY_DURATION\",\n    floor(datediff(\"month\", col(\"POLICY_START_DATE\"), col(\"INCIDENT_DATE\"))))\n# Let's all drop the date fields and the age field, as we did in part 1\ndata_combined_df = data_combined_df.drop(\"age\", \"INCIDENT_DATE\", \"POLICY_START_DATE\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e7635cb-73d3-4f55-8fb6-0a7c366b6f20",
   "metadata": {
    "language": "python",
    "name": "Full_Data_df",
    "collapsed": false
   },
   "outputs": [],
   "source": "print(f\"row count: {data_combined_df.count()}\")\nst.dataframe(data_combined_df.limit(50))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11d48aca-5e82-4a25-9c7f-5b183098e33d",
   "metadata": {
    "name": "Markdown2",
    "collapsed": false
   },
   "source": "## 2. Pre-process data and import model"
  },
  {
   "cell_type": "code",
   "id": "2b3e7a02-dd06-4dc3-9814-4f03a79e49ff",
   "metadata": {
    "language": "python",
    "name": "Import_Model",
    "collapsed": false
   },
   "outputs": [],
   "source": "# From the Model Registry, let's pull down the model we trained previous\nreg = Registry(session=session, database_name=\"CONTAINER_RUNTIME_LAB\", schema_name=\"DATA\")\nxgb_gs_fraud_model = reg.get_model(\"xgb_gs_fraud_model\").last() #or we can use .version(<version_name>)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85d9cc65-9464-4749-8516-d9a990ba8809",
   "metadata": {
    "language": "python",
    "name": "Run_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "new_predictions = xgb_gs_fraud_model.run(data_combined_df, function_name='PREDICT')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98d08b3f-357e-41d0-b988-4210033ec5ca",
   "metadata": {
    "language": "python",
    "name": "Data_Split",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "original_data = new_predictions.filter(col('SOURCE')==\"ORIGINAL\")\nnew_data = new_predictions.filter(col('SOURCE')==\"NEW\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47f69f83-3794-4e0c-a119-d91c6be3827e",
   "metadata": {
    "language": "python",
    "name": "ACC_Score_Org",
    "collapsed": false
   },
   "outputs": [],
   "source": "#feature_columns = scaled_features + encoded_features + ordinal_encoded_features + other_features\nlabel_column = ['FRAUD_REPORTED']\noutput_column = ['PREDICTED_FRAUD']\n\nACCURACY_ORIGINAL = accuracy_score(df=original_data, y_true_col_names=label_column, y_pred_col_names=output_column)\nAUC_ORIGINAL = roc_auc_score(df=original_data, y_true_col_names=label_column, y_score_col_names=output_column)\n\nprint(f'Acccuracy (Original Data): {ACCURACY_ORIGINAL}')\nprint(f'AUC (Original Data): {AUC_ORIGINAL}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04552d4b-41e6-44f9-a2c2-30c4b3c3a9dd",
   "metadata": {
    "language": "python",
    "name": "ACC_Score_Set_Value",
    "collapsed": false
   },
   "outputs": [],
   "source": "xgb_gs_fraud_model.set_metric(\"Evaluation_Info\", {'Acc': ACCURACY_ORIGINAL, 'Acc': AUC_ORIGINAL})",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41343d0e-616d-434e-82e1-49c24dfad1c1",
   "metadata": {
    "language": "python",
    "name": "Model_Show_Versions",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "reg.get_model('xgb_gs_fraud_model').show_versions()\n#xgb_gs_fraud_model.show_metrics()['Acc']",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff4d1e65-95cf-4ecb-a9b6-1e62fd6adecb",
   "metadata": {
    "language": "python",
    "name": "ACC_Score_New",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "ACCURACY = accuracy_score(df=new_data, y_true_col_names=label_column, y_pred_col_names=output_column)\nAUC = roc_auc_score(df=new_data, y_true_col_names=label_column, y_score_col_names=output_column)\nprint(f'Acccuracy (New Data): {ACCURACY}')\nprint(f'AUC (New Data): {AUC}')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "460cda8e-6ebe-4648-808b-32b6135fdb54",
   "metadata": {
    "name": "Markdown4",
    "collapsed": false
   },
   "source": "## 4. Examine PSI and KDE across predictions and features"
  },
  {
   "cell_type": "code",
   "id": "682872ed-5d6e-4976-b992-231b4d0bd341",
   "metadata": {
    "language": "python",
    "name": "PSI_Def",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def calculate_psi(expected_array, actual_array, buckets=3):\n    # Remove casting to int\n    expected_array = expected_array.astype(float)\n    actual_array = actual_array.astype(float)\n    \n    # Use equal-width bins\n    breakpoints = np.linspace(np.min(expected_array), np.max(expected_array), buckets + 1)\n    \n    expected_counts = np.histogram(expected_array, bins=breakpoints)[0]\n    actual_counts = np.histogram(actual_array, bins=breakpoints)[0]\n    \n    # Apply smoothing\n    expected_percents = expected_counts / expected_counts.sum()\n    actual_percents = actual_counts / actual_counts.sum()\n    \n    # Replace zeros to avoid division by zero or log of zero\n    expected_percents = np.where(expected_percents == 0, 1e-6, expected_percents)\n    actual_percents = np.where(actual_percents == 0, 1e-6, actual_percents)\n    \n    psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n    total_psi = np.sum(psi_values)\n    return total_psi",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ac3a047-66aa-4cd0-8004-2b95ab16e327",
   "metadata": {
    "language": "python",
    "name": "Save_Data",
    "collapsed": false
   },
   "outputs": [],
   "source": "#reference_data = session.read.table(\"CONTAINER_RUNTIME_LAB.DATA.REFERENCE_DATA\").to_pandas()\noriginal_data.write.save_as_table('CONTAINER_RUNTIME_LAB.DATA.REFERENCE_DATA', mode='overwrite')\nnew_data.write.save_as_table('CONTAINER_RUNTIME_LAB.DATA.CURRENT_DATA', mode='overwrite')\n\noriginal_data_pd = original_data.to_pandas()\nnew_predictions_pd = new_data.to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cca76ab-cada-425c-8dcc-2eedc858febc",
   "metadata": {
    "language": "python",
    "name": "KDE_PSI_Label",
    "collapsed": false
   },
   "outputs": [],
   "source": "plt.figure(figsize=(4, 3))\nsns.set_context(\"notebook\")\nsns.kdeplot(data=original_data_pd['FRAUD_REPORTED'], label='training_data', fill=True, color='blue', common_norm=False)\nsns.kdeplot(data=new_predictions_pd['FRAUD_REPORTED'], label='new_data', fill=True, color='red', common_norm=False)\nplt.title('Kernel Density Estimate of \"FRAUD_REPORTED\" for Training Data and New Data')\nplt.xlabel('Likelihood of Fraud')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\npsi = calculate_psi(original_data_pd['FRAUD_REPORTED'],new_predictions_pd['FRAUD_REPORTED'])\nprint(f\"Fraud PSI: {psi}\")\n\n# For Population Stability Index, this is generally the metrics we look at to determine if significant drift has occured.\n# PSI < 0.1: No significant change\n# 0.1 ≤ PSI < 0.2: Moderate change\n# PSI ≥ 0.2: Significant change",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa659d6f-a006-4c26-975e-55f59e8df71f",
   "metadata": {
    "language": "python",
    "name": "Hist_dist",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Given we are looking at binary data, We can also simply observe the proportion of fraud across two datasets, normalized\ncombined_data = pd.concat([original_data_pd, new_predictions_pd])\n\n# Calculate proportions\nproportions = combined_data.groupby(['SOURCE', 'FRAUD_REPORTED']).size().reset_index(name='Count')\nproportions['Proportion'] = proportions.groupby('SOURCE')['Count'].transform(lambda x: x / x.sum())\n\n# Plotting with Seaborn\nsns.barplot(x='FRAUD_REPORTED', y='Proportion', hue='SOURCE', data=proportions)\nplt.xlabel('Likelihood of Fraud')\nplt.ylabel('Proportion')\nplt.title('Proportion of \"FRAUD_REPORTED\" for Training Data and New Data')\nplt.legend()\nplt.show()\n\npsi = calculate_psi(original_data_pd['FRAUD_REPORTED'],new_predictions_pd['FRAUD_REPORTED'])\nprint(f\"Fraud PSI: {psi}\")\n\n# For Population Stability Index, this is generally the metrics we look at to determine if significant drift has occured.\n# PSI < 0.1: No significant change\n# 0.1 ≤ PSI < 0.2: Moderate change\n# PSI ≥ 0.2: Significant change",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f874ec6c-369a-4950-ac64-6bea996a8291",
   "metadata": {
    "language": "python",
    "name": "Calc_PSI",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's get a list of columns, assuming both lists have the same fields. Let's also exclude POLICY_NUMBER\ncolumns = (col for col in original_data_pd.columns if col not in ['POLICY_NUMBER'])\n\n# List to store PSI values\npsi_values = []\n\nfor column in columns:\n    # Check if the column is numeric\n    if np.issubdtype(original_data_pd[column].dtype, np.number):\n        psi = calculate_psi(original_data_pd[column], new_predictions_pd[column])\n        psi_values.append((column, psi))\n    else:\n        st.write(f\"Skipping {column} - not numeric or boolean\")\n# Sort PSI values from highest to lowest\nsorted_psi = sorted(psi_values, key=itemgetter(1), reverse=True)\n\n# Create a DataFrame for easier manipulation\npsi_df = pd.DataFrame(sorted_psi, columns=['Column', 'PSI']).set_index('Column')\n\n# Display the top 5 columns with the highest PSI values using Streamlit\nst.title(\"Top 5 Features with Highest PSI Values\")\nst.table(psi_df.head(5).style.format({'PSI': '{:.3f}'}))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "781c2816-57b1-4948-9918-859822069e59",
   "metadata": {
    "language": "python",
    "name": "KDE_PSI_2",
    "collapsed": false
   },
   "outputs": [],
   "source": "plt.figure(figsize=(4, 3))\nsns.set_context(\"notebook\")\nsns.kdeplot(data=original_data_pd['INCIDENT_HOUR_OF_THE_DAY'], label='training_data', fill=True, color='blue')\nsns.kdeplot(data=new_predictions_pd['INCIDENT_HOUR_OF_THE_DAY'], label='new_data', fill=True, color='red')\nplt.title('Kernel Density Estimate of \"INCIDENT_HOUR_OF_THE_DAY\" for Training Data and New Data')\nplt.xlabel('Likelihood of Fraud')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\npsi = calculate_psi(original_data_pd['INCIDENT_HOUR_OF_THE_DAY'],new_predictions_pd['INCIDENT_HOUR_OF_THE_DAY'])\nprint(f\"Fraud PSI: {psi}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c119a9b0-900d-44a7-be2e-5b66ce67495d",
   "metadata": {
    "name": "Markdown5",
    "collapsed": false
   },
   "source": "## 5. Use Evidently to generate reports and automatically push to Streamlit + Email"
  },
  {
   "cell_type": "code",
   "id": "a86bc778-0979-4b0a-8271-e942136c20fc",
   "metadata": {
    "language": "python",
    "name": "Helper_Generate_Email",
    "collapsed": false
   },
   "outputs": [],
   "source": "def generate_sql_email_message(data: dict, file: str) -> str:\n    from datetime import datetime\n    import pandas as pd\n    import pytz\n\n    tz = pytz.timezone('Europe/London')\n    current_timestamp = datetime.now(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n   \n    # Generate summary \n    summary = data.get('summary', {})\n    if not summary:\n        return \"Error: 'summary' key not found in data\"\n\n    # Generate table with column test status\n    df = pd.DataFrame(data.get('tests', []))\n    if not df.empty:\n        df['column_name'] = df['parameters'].apply(lambda x: x.get('column_name', 'N/A'))\n        df = df[['column_name', 'status']]\n        table_html = df.to_html(index=False).replace(\"'\", '\"')\n    else:\n        table_html = \"<p>No test data available</p>\"\n    \n    email_content = f\"\"\"\n    Date: {current_timestamp} <br>\n    Successful tests: {summary.get('success_tests', 0)} <br>\n    Failed tests: {summary.get('failed_tests', 0)} <br>\n    Report: <a href=\"{file}\">Download Report</a>\n    \n    {table_html}\n    \"\"\"\n    # Remove non-ASCII characters\n    email_content_ascii = email_content.encode('ascii', 'ignore').decode('ascii')\n    # Escape single quotes\n    email_content_escaped = email_content_ascii.replace(\"'\", \"''\")\n\n    sql = f\"\"\"\n    CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(\n        SNOWFLAKE.NOTIFICATION.TEXT_HTML('{email_content_escaped}'),\n        SNOWFLAKE.NOTIFICATION.EMAIL_INTEGRATION_CONFIG(\n            'my_email_int',\n            'Drift Detection Report {current_timestamp}',\n            ARRAY_CONSTRUCT('harley.chen@snowflake.com')\n        )\n    )\n    \"\"\"\n    return sql\n\n@sproc(session=session, name='evidently_monitor', stage_location='@MONITORING',  \n       packages=['snowflake-snowpark-python', 'pandas', 'evidently', 'snowflake-ml-python', 'tabulate', 'pytz'],\n       is_permanent=True, \n       replace=True)\ndef monitor_model(session: Session) -> dict:\n    from evidently.test_preset import DataDriftTestPreset\n    from evidently.test_suite import TestSuite\n    from datetime import datetime\n    import pandas as pd\n    import pytz\n    import traceback\n    import json\n    \n    output = {}\n    tz = pytz.timezone('Europe/London')\n    feature_columns = ['INCIDENT_HOUR_OF_THE_DAY', \n                       '\"\"\"INSURED_OCCUPATION_encoded_handlers-cleaners\"\"\"',\n                       '\"\"\"INSURED_OCCUPATION_encoded_prof-specialty\"\"\"',\n                       'POLICY_DEDUCTABLE_SCALED'] # Just picking the top columns from the Calc_PSI cell.\n                                                   # You can include all fields.\n    # Load reference data\n    reference = session.table(\"CONTAINER_RUNTIME_LAB.DATA.REFERENCE_DATA\").select(feature_columns).to_pandas()\n    output['reference_head'] = reference.head(5).to_dict()\n    # Load current data\n    current = session.table('CONTAINER_RUNTIME_LAB.DATA.CURRENT_DATA').select(feature_columns).to_pandas()\n    output['current_head'] = current.head(5).to_dict()\n    \n    try:\n        # Generate the report\n        report = TestSuite(tests=[DataDriftTestPreset(stattest=\"psi\", stattest_threshold=0.3)])\n        report.run(reference_data=reference, current_data=current)\n    except Exception as e:\n        error_msg = f\"Error generating report: {str(e)}\\n{traceback.format_exc()}\"\n        print(error_msg)\n        return {'Error': error_msg}\n\n    try:\n        # Upload report to stage\n        timestamp = datetime.now(tz)\n        timestamp1 = timestamp.strftime('%Y_%m_%d_%H_%M_%S')\n        timestamp2 = timestamp.strftime('%Y/%m/%d/')\n        filename = f\"/tmp/{timestamp1}.html\"\n        stage_filename = f\"@MONITORING/report/{timestamp2}\"\n        report.save_html(filename)\n        session.file.put(filename, stage_filename, auto_compress=False, overwrite=True)\n        download_url_query = f\"SELECT GET_PRESIGNED_URL('@MONITORING', 'report/{timestamp2}{timestamp1}.html') AS DOWNLOAD_LINK\"\n        download_url = session.sql(download_url_query).collect()[0]['DOWNLOAD_LINK']\n    except Exception as e:\n        error_msg = f\"Error uploading report: {str(e)}\\n{traceback.format_exc()}\"\n        print(error_msg)\n        return {'Error': error_msg}\n    \n    try:\n        test_summary = report.as_dict()\n        print(f\"test_summary keys: {test_summary.keys()}\")\n    except Exception as e:\n        error_msg = f\"Error converting report to dict: {str(e)}\\n{traceback.format_exc()}\"\n        print(error_msg)\n        return {'Error': error_msg}\n\n    # Send an email if any tests failed\n    if test_summary.get('summary', {}).get('failed_tests', 0) > 0:\n        try:\n            sql_query = generate_sql_email_message(test_summary, download_url)\n            print(f\"Generated SQL query: {sql_query}\")\n            session.sql(sql_query).collect()\n        except Exception as e:\n            error_msg = f\"Error sending email: {str(e)}\\n{traceback.format_exc()}\"\n            print(error_msg)\n            return {'Error': error_msg}\n\n    result = {**test_summary, **output}\n    print(f\"Final result keys: {result.keys()}\")\n    return result\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65808a2e-9a3b-4470-9f79-1dae24258bbd",
   "metadata": {
    "language": "python",
    "name": "Evidently_Show_Results",
    "collapsed": false
   },
   "outputs": [],
   "source": "import json\n\ndrift_detection_results_str = monitor_model(session)\ndrift_detection_results = json.loads(drift_detection_results_str)\ndrift_detection_results['summary']",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "654a0fc3-7b60-44d4-9061-71349a9a5930",
   "metadata": {
    "language": "python",
    "name": "Plot_Def",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def plot_evidently_results(drift_detection_results):\n    # Data extraction (keep as is)\n    summary_data = drift_detection_results['summary']\n    test_data = pd.DataFrame(drift_detection_results['tests'])\n    feature_drift = []\n    for test in drift_detection_results['tests']:\n        if 'parameters' in test and 'features' in test['parameters']:\n            for feature, params in test['parameters']['features'].items():\n                feature_drift.append({\n                    'feature': feature,\n                    'detected': params['detected'],\n                    'score': params['score'],\n                    'stattest': params['stattest'],\n                    'threshold': params['threshold']\n                })\n    feature_drift_df = pd.DataFrame(feature_drift)\n\n    # Plotting\n    fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n\n    # Summary plot\n    ax[0].bar(summary_data['by_status'].keys(), summary_data['by_status'].values(), color=['#FF9999', '#66B2FF'])\n    ax[0].set_title('Test Summary', fontsize=16)\n    ax[0].set_ylabel('Count', fontsize=12)\n    ax[0].set_xlabel('Status', fontsize=12)\n    ax[0].tick_params(axis='both', which='major', labelsize=10)\n\n    # Drift scores bar plot\n    feature_drift_df.plot(kind='bar', x='feature', y='score', ax=ax[1], color='#66B2FF')\n    ax[1].set_title('Feature Drift Scores', fontsize=16)\n    ax[1].set_ylabel('Drift Score', fontsize=12)\n    ax[1].set_xlabel('Feature', fontsize=12)\n    ax[1].tick_params(axis='both', which='major', labelsize=10)\n    plt.setp(ax[1].get_xticklabels(), rotation=45, ha='right')\n\n    # Pie chart for drifted features\n    drifted_counts = feature_drift_df['detected'].value_counts()\n    wedges, texts, autotexts = ax[2].pie(drifted_counts, labels=['No Drift', 'Drift'], \n                                         autopct='%1.1f%%', colors=['#66B2FF', '#FF9999'],\n                                         textprops={'fontsize': 10})\n    ax[2].set_title('Proportion of Drifted Features', fontsize=16)\n\n    plt.tight_layout(pad=0.5)\n    plt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f9868ea-0af9-46b9-a258-fe27e66cb997",
   "metadata": {
    "language": "python",
    "name": "Plot",
    "collapsed": false
   },
   "outputs": [],
   "source": "# We can also visualize the results from the drift detection\nplot_evidently_results(drift_detection_results)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30ff5d5f-33bb-4156-848d-b3f139d13310",
   "metadata": {
    "language": "python",
    "name": "Select_all_Data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import LongType\n\n# Note that I have combined the data to rerun our model, since the new data is not large and credible enough to train a new model on its own.\n# If your new data is credible enough, you can simply just take the new dataset and retrain the model only on the new dataset.\ntrain_data_all, test_data_all = data_combined_df.drop('source').random_split(weights = [0.8, 0.2], seed = 42) \n\n\ntrain_data_all = train_data_all.with_column(\"FRAUD_REPORTED\", col(\"FRAUD_REPORTED\").astype(LongType()))\ntest_data_all = test_data_all.with_column(\"FRAUD_REPORTED\", col(\"FRAUD_REPORTED\").astype(LongType()))\n\n# Determine the label column name\nlabel_column_all = ['FRAUD_REPORTED']\noutput_column_all = ['PREDICTED_FRAUD']",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fbf66bf-88c2-4720-bf01-91a718f874e2",
   "metadata": {
    "language": "python",
    "name": "Pipeline_Generation",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Define the categories with their specific order\ncategories = {\n    \"INSURED_EDUCATION_LEVEL\": np.array([\"High School\", \"Associate\", \"College\", \"Masters\", \"JD\", \"MD\", \"PhD\"]),\n    \"INCIDENT_SEVERITY\": np.array([\"Trivial Damage\", \"Minor Damage\", \"Major Damage\", \"Total Loss\"])\n}\n# Create the OrdinalEncoder with specified categories\nOrdinalEncoding = OrdinalEncoder(\n    input_cols=[\"INSURED_EDUCATION_LEVEL\", \"INCIDENT_SEVERITY\"],\n    output_cols=[\"INSURED_EDUCATION_LEVEL_OE\", \"INCIDENT_SEVERITY_OE\"],\n    categories=categories,\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=-1,\n    drop_input_cols=True\n)\n\n# Define the columns to encode\ncolumns_to_encode = [\n    \"INSURED_SEX\",\n    \"INSURED_OCCUPATION\",\n    \"INCIDENT_TYPE\",\n    \"AUTHORITIES_CONTACTED\",\n    \"POLICE_REPORT_AVAILABLE\"\n]\n# Create a OneHotEncoder instance\nOneHotEncoding = OneHotEncoder(\n    input_cols=columns_to_encode,\n    output_cols=[f\"{col}_encoded\" for col in columns_to_encode],\n    drop_input_cols=True,  # Keep original columns\n    handle_unknown='ignore'  # Ignore any unknown categories during transform\n)\n\n# Define the columns to scale\ncolumns_to_scale = [\n    'POLICY_LENGTH_MONTH',\n    'POLICY_DEDUCTABLE',\n    'POLICY_ANNUAL_PREMIUM',\n    'CLAIM_AMOUNT',\n    'POLICY_DURATION'\n]\n# Create the StandardScaler\nStandardScaling = StandardScaler(\n    input_cols=columns_to_scale,\n    output_cols=[f\"{col}_SCALED\" for col in columns_to_scale],\n    with_mean=True,\n    with_std=True,\n    drop_input_cols=True  # Keep original columns\n)\n\n# Determine the label column name\n# feature_columns = train_data.columns.remove('FRAUD_REPORTED_LONG')\nlabel_column = ['FRAUD_REPORTED']\noutput_column = ['PREDICTED_FRAUD']\n\n# # Initially, we can run this under the XGB Classifier model. However, you will notice that\n# # the model overfits on the training data and performs poorly on the test dataset\n# xgbmodel = XGBClassifier(\n#     random_state=1, \n#     #input_cols=feature_columns,    #here we are passing all columns so we have commented out. If you have specific columns set as features, you should specify them here\n#     label_cols=label_column,\n#     output_cols=output_column\n#     )\n\nxgb_grid_search = GridSearchCV(\n    estimator=XGBClassifier(),\n    param_grid={\n        \"n_estimators\":[10, 20, 30, 50, 100, 150, 200, 250, 300],\n        \"subsample\": [0.9, 0.5, 0.2],\n        \"max_depth\": range(2,10,1),\n        \"learning_rate\":[0.1, 0.06, 0.05, 0.03, 0.01, 0.005, 0.002, 0.001],\n    },\n    n_jobs = -1,\n    #input_cols=feature_columns,    #here we are passing all columns so we have commented out. \n                                    #If you have specific columns set as features, you should specify them here\n    label_cols=label_column,\n    output_cols=output_column,\n)\n\n# xgb_gs_fitted = xgb_grid_search.fit(train_data)\n\nmodel_pipeline = Pipeline(\n    steps=[\n        (\"Ordinal_encoding\",OrdinalEncoding),\n        (\"OneHotEncoding\",OneHotEncoding),\n        (\"standardscaler\",StandardScaling),\n        #(\"XGBClassifier\", xgbmodel)\n        (\"CV_XGBClassifier\", xgb_grid_search)\n    ]\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "101974bd-2410-4ac6-8b3b-edf856fbdd81",
   "metadata": {
    "language": "python",
    "name": "WH_Up",
    "collapsed": false
   },
   "outputs": [],
   "source": "wh = str(session.get_current_warehouse()).strip('\"')\nprint(f\"Current warehouse: {wh}\")\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n\nsession.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = LARGE\").collect()\n\n#Give Snowflake a few seconds to change WH sizes\nimport time\ntime.sleep(5)\n\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1859b9f-399f-4994-b36d-2f9476e548d1",
   "metadata": {
    "language": "python",
    "name": "Retrain_Model",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Typically trains for about 3mins in a large wh\nxgb_gs_fitted = model_pipeline.fit(train_data_all)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "630b2d2f-8148-47a1-a9f8-1fa0091c1551",
   "metadata": {
    "language": "python",
    "name": "WH_Down",
    "collapsed": false
   },
   "outputs": [],
   "source": "wh = str(session.get_current_warehouse()).strip('\"')\nprint(f\"Current warehouse: {wh}\")\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n\nsession.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = XSMALL\").collect()\n\n#Give Snowflake a few seconds to change WH sizes\nimport time\ntime.sleep(5)\n\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92783eb2-fc12-49b1-bd86-2f3b22ff5937",
   "metadata": {
    "language": "python",
    "name": "New_Model_Pred",
    "collapsed": false
   },
   "outputs": [],
   "source": "gb_gs_train = xgb_gs_fitted.predict(train_data_all)\nxgb_gs_predictions = xgb_gs_fitted.predict(test_data_all)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95efdc55-d87c-413c-91fb-671e363590ab",
   "metadata": {
    "language": "python",
    "name": "New_Model_Acc",
    "collapsed": false
   },
   "outputs": [],
   "source": "ACCURACY_NEW = accuracy_score(df=xgb_gs_predictions, y_true_col_names=label_column_all, y_pred_col_names=output_column_all)\nprint(f'Acccuracy (New Data): {ACCURACY}')\nprint(f'Acccuracy (Retrained Model on Test Data): {ACCURACY_NEW}')\n\n\nAUC_NEW = roc_auc_score(df=xgb_gs_predictions, y_true_col_names=label_column, y_score_col_names=output_column)\nprint(f'AUC (New Data): {AUC}')\nprint(f'AUC (Retrained Model on Test Data): {AUC_NEW}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe92515f-c388-4606-9ab2-999accac2f96",
   "metadata": {
    "language": "python",
    "name": "helper_mod_ver",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# FUNCTION used to iterate the model version so we can automatically \n# create the next version number\nimport ast\nimport builtins  # Import the builtins module\n#from snowflake.snowpark import functions as F \n\ndef get_next_version(reg, model_name) -> str:\n    \"\"\"\n    Returns the next version of a model based on the existing versions in the registry.\n\n    Args:\n        reg: The registry object that provides access to the models.\n        model_name: The name of the model.\n\n    Returns:\n        str: The next version of the model in the format \"V_\".\n\n    Raises:\n        ValueError: If the version list for the model is empty or if the version format is invalid.\n    \"\"\"\n    models = reg.show_models()\n    if models.empty:\n        return \"V_1\"\n    elif model_name not in models[\"name\"].to_list():\n        return \"V_1\"\n    max_version_number = builtins.max(  \n        [\n            int(version.split(\"_\")[-1])\n            for version in ast.literal_eval(\n                models.loc[models[\"name\"] == model_name, \"versions\"].values[0]\n            )\n        ]\n    )\n    return f\"V_{max_version_number + 1}\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "777f0eea-9afb-4d3e-882a-0a04f3a033fc",
   "metadata": {
    "name": "Markdown6",
    "collapsed": false
   },
   "source": "## 6. Retrain the model based on new + old data; register model if Accuracy improves"
  },
  {
   "cell_type": "code",
   "id": "2b5f2db3-37ac-4e7b-92e0-01277852fdae",
   "metadata": {
    "language": "python",
    "name": "Def_train_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "def train_model(session:Session, reg, new_model, model_name, acc_metric_old, acc_metric_new) -> str:\n    model_version = get_next_version(reg, model_name)\n    \n    if acc_metric_old < acc_metric_new:\n            # Set new mode las default model\n        registered_model = reg.log_model(\n            new_model,\n            model_name=model_name,\n            version_name=model_version,\n            conda_dependencies=[\"snowflake-ml-python\"],\n            comment=\"Model trained using GridsearchCV in Snowpark to predict fraud claims\",\n            metrics={\"Acc\": acc_metric_new},\n            options= {\"relax_version\": False}\n        )\n        reg.get_model(model_name).default = model_version\n        return f\"Registered new model with version {registered_model.version_name} as the performance has imporved \\nPrevious Accuracy Metric: {acc_metric_old}\\nNew Accuracy Metric: {acc_metric_new}\"\n    else:\n        return f\"Model not updated as the model accuracy has not meaningfully improved.\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2cc06d40-383b-4f0e-b506-fc50611d4c28",
   "metadata": {
    "language": "python",
    "name": "Train_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "train_model(Session, reg, xgb_gs_fitted, 'XGB_GS_FRAUD_MODEL', ACCURACY, ACCURACY_NEW)",
   "execution_count": null
  }
 ]
}