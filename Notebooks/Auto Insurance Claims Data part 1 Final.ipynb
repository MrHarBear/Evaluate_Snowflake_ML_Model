{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc441053-2441-45a9-8586-cebc7c5db910",
   "metadata": {
    "name": "Title",
    "collapsed": false
   },
   "source": "#  **SWT 2024 Evaluating your Machine Learning Models in Snowflake**\n### Notebook 1 - Data Ingestion\n---\n### What We'll Do:\n1. **Data Ingestion**: Fetch customer and claims data from our database\n2. **Data Transformation**: Utilize Snowpark DataFrames for data preparation and analysis\n3. **Model Training**: Train a XGB Classifier model\n4. **Model Registry**: Saving the model to Snowflake Model Registry\n\nRemember to add the necessary packages in the 'Packages' drop down at the top. For example,\n- `snowflake-snowpark-python`\n- `snowflake-ml-python`\n- `pandas`\n- etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1940f-fd92-4f0e-8934-64c548f61448",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Import_Packages"
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode\n)\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark.functions import *\nfrom snowflake.snowpark.types import LongType\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import *\nfrom snowflake.ml.registry import Registry\n\n# Import python packages\nimport streamlit as st\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport streamlit as st\nimport json\nimport tabulate\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64277a92-71ae-41c2-bd63-869a5d752f27",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "setting"
   },
   "outputs": [],
   "source": [
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.ml import version\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(VERSION[0],VERSION[1],VERSION[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(version.VERSION[0],version.VERSION[2],version.VERSION[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50437e40-bbf8-47e1-a976-4e785d7ecc10",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "read_data"
   },
   "outputs": [],
   "source": "claim_data = session.read.table(\"SWT2024_DEMO_AUTO_INSURANCE.DATA.CLAIM_DATA\")\ncustomer_data = session.read.table(\"SWT2024_DEMO_AUTO_INSURANCE.DATA.CUSTOMER_DATA\")\n\nst.dataframe(claim_data.limit(50))\nst.dataframe(customer_data.limit(50))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38798ef3-ef80-42b1-995f-e1949e578877",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Claim_Data_Cleanse"
   },
   "outputs": [],
   "source": "claim_data = claim_data.replace('?', None)\nclaim_data.filter(col(\"POLICE_REPORT_AVAILABLE\").is_null()).show(10)\n# Calculate the mode of the 'POLICE_REPORT_AVAILABLE' column\nmode_value = claim_data.select(mode(col(\"POLICE_REPORT_AVAILABLE\"))).collect()[0][0]\nprint(f\"Fill NULL value in POLICY_REPORT_AVAIALLBE to the mode: {mode_value}\")\n# Fill NULL values with the mode\nclaim_data = claim_data.with_column(\"POLICE_REPORT_AVAILABLE\", \n    when(col(\"POLICE_REPORT_AVAILABLE\").is_null(), mode_value)\n    .otherwise(col(\"POLICE_REPORT_AVAILABLE\")))\nclaim_data.filter(col(\"POLICE_REPORT_AVAILABLE\").is_null()).show(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "fs_create"
   },
   "outputs": [],
   "source": [
    "database = 'SWT2024_DEMO_AUTO_INSURANCE'\n",
    "schema = 'DATA'\n",
    "warehouse = 'DEMO_WH'\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=database,\n",
    "    name=schema,\n",
    "    default_warehouse=warehouse,\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25661785-8644-4ffe-81a5-4bf19de4cc76",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "fs_register_entity"
   },
   "outputs": [],
   "source": [
    "# Snowflake Feature Store requires an \"entity\" with \"join_keys\" be registered\n",
    "POLICY_NUMBER = Entity(name=\"POLICY_NUMBER\", join_keys=[\"POLICY_NUMBER\"])\n",
    "fs.register_entity(POLICY_NUMBER)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2039504-c250-4f67-ad4e-8459ef2b8d9f",
   "metadata": {
    "language": "python",
    "name": "llm_col_desc",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.cortex import Complete\n\n\nllm = 'llama3-70b'\n\nprompt = f\"\"\"\nGiven the SQL Code for selecting the dataframe: {customer_data.queries['queries'][0]}\nA sample of the dataframe: {customer_data.sample(n=30).to_pandas().to_markdown()}. \nDescribe the following features of the data given succinctly: ['AGE', 'POLICY_START_DATE', 'POLICY_LENGTH_MONTH', 'POLICY_DEDUCTABLE', 'POLICY_ANNUAL_PREMIUM', 'INSURED_SEX', 'INSURED_EDUCATION_LEVEL', 'INSURED_OCCUPATION'] \nFor context, this is customer and policy information about individuals that have insurance with the company.\nThe descriptions will be stored in a feature store in Snowflake. Return a JSON where the feature name is the key and the description is the value.\n\"\"\"\nllm_response = Complete(llm, prompt)\n\nfeature_desc = json.loads(llm_response.split('```')[1])\nfor key in feature_desc:\n    feature_desc[key] = feature_desc[key].replace(\"'\", '')\nfeature_desc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d034b32-cae9-47be-b6ab-af0d5fbc1e29",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "fs_featureview"
   },
   "outputs": [],
   "source": "fv = FeatureView(\n    name=\"customer_data\",\n    entities = [POLICY_NUMBER],\n    feature_df = customer_data,\n    #refresh_freq=\"1 hour\",  # can also be a cron schedule - * * * * * America/Los_Angeles\n    desc=\"Insurance Customer Data\")\n\nfv = fv.attach_feature_desc(feature_desc)\n\n#Let's register this FeatureView in Snowflake \nregistered_fv = fs.register_feature_view(\n    feature_view=fv,\n    version=\"V1\",\n    overwrite = True\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adce047-33a6-4e13-a5b2-b57914b61c83",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "to_delete_fv"
   },
   "outputs": [],
   "source": "# fv = FeatureView(\n#    name=\"customer_data\",\n#    entities = [POLICY_NUMBER],\n#    feature_df=customer_data,    \n# )\n# registered_fv = fs.register_feature_view(\n#    feature_view=fv,\n#    version=\"V2\"\n# )\n# fs.delete_feature_view(registered_fv)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b762a89-f620-4516-a5cb-ec2011e626c9",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "generate_dataset"
   },
   "outputs": [],
   "source": "customer_fv = fs.get_feature_view(\n    name = 'customer_data',\n    version = 'V1'\n)\n\ntraining_data = fs.generate_dataset(\n    name=\"Harleytest\",\n    spine_df=claim_data,\n    features=[customer_fv],\n    spine_label_cols = [\"FRAUD_REPORTED\"]\n)\n\ntraining_data_df = training_data.read.to_snowpark_dataframe()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8e49c-c07c-4385-b9ae-1da1de6f4367",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "policy_duration"
   },
   "outputs": [],
   "source": "# Understand the policy duration from the policy start date to the indicent date\nfrom snowflake.snowpark.functions import col\n\ntraining_data_df = training_data_df.with_column(\"POLICY_DURATION\",\n    floor(datediff(\"month\", col(\"POLICY_START_DATE\"), col(\"INCIDENT_DATE\"))))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982b0dc-69f1-4c03-b2a5-8e372920fda7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "corr_calc"
   },
   "outputs": [],
   "source": "# Convert Snowpark DataFrame to pandas DataFrame\npandas_df = training_data_df.to_pandas()\n\n# Select only numeric columns\nnumeric_columns = pandas_df.select_dtypes(include=['int64', 'float64']).columns\nnumeric_df = pandas_df[numeric_columns]\n\n# Calculate correlation matrix\ncorr_matrix = numeric_df.corr()\nsns.set(font_scale=0.5)\n# Create heatmap\nplt.figure(figsize=(7, 5))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, center=0)\nplt.title(\"Correlation Heatmap of Claim Data\", fontdict = {'fontsize' : 12})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Drop_Unneeded_Fields"
   },
   "outputs": [],
   "source": "# Due to the high correlation between Age and policy_length_month, let's drop age.\n# Let's all drop the date fields \ntraining_data_df = training_data_df.drop(\"age\", \"INCIDENT_DATE\", \"POLICY_START_DATE\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd2ad3-7207-45a1-91cf-63cbaf3c237e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "check_unique_values"
   },
   "outputs": [],
   "source": "# Now, we want to start to encode all categorical variable and turning all the strings into numeric fields so our model can train on them\n# Select only string columns (equivalent to 'object' dtype in pandas)\ncategorical_claim_data = training_data_df.select([col for col in training_data_df.columns if isinstance(training_data_df.schema[col].datatype, StringType)])\n\n# Print unique values for each column\nfor col in categorical_claim_data.columns:\n    unique_values = categorical_claim_data.select(col).distinct().collect()\n    unique_list = [row[col] for row in unique_values]\n    print(f\"{col}:\")\n    print(unique_list)\n    print()  # Add a blank line for readability"
  },
  {
   "cell_type": "code",
   "id": "0f07bfa9-194c-4704-95ff-759a60b689f7",
   "metadata": {
    "language": "python",
    "name": "Split_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\n\ntraining_data_df = training_data_df.with_column(\"FRAUD_REPORTED\", col(\"FRAUD_REPORTED\").astype(LongType()))\ntrain_data, test_data = training_data_df.random_split(weights = [0.8, 0.2], seed = 42)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7185c31b-627b-49d0-8a9e-10a174f8838d",
   "metadata": {
    "language": "python",
    "name": "Save_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "train_data.write.save_as_table('SWT2024_DEMO_AUTO_INSURANCE.DATA.TRAIN_DATA', mode = 'overwrite')\ntest_data.write.save_as_table('SWT2024_DEMO_AUTO_INSURANCE.DATA.TRAIN_DATA', mode = 'overwrite')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e79a7689-2e7e-44d4-9083-e8167ece35f8",
   "metadata": {
    "language": "python",
    "name": "Train_data_Sample"
   },
   "outputs": [],
   "source": "st.dataframe(train_data.limit(50))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88ab04-747c-4122-bfb9-4f196cf0e4a6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Pipeline_Generation"
   },
   "outputs": [],
   "source": "# Define the categories with their specific order\ncategories = {\n    \"INSURED_EDUCATION_LEVEL\": np.array([\"High School\", \"Associate\", \"College\", \"Masters\", \"JD\", \"MD\", \"PhD\"]),\n    \"INCIDENT_SEVERITY\": np.array([\"Trivial Damage\", \"Minor Damage\", \"Major Damage\", \"Total Loss\"])\n}\n# Create the OrdinalEncoder with specified categories\nOrdinalEncoding = OrdinalEncoder(\n    input_cols=[\"INSURED_EDUCATION_LEVEL\", \"INCIDENT_SEVERITY\"],\n    output_cols=[\"INSURED_EDUCATION_LEVEL_OE\", \"INCIDENT_SEVERITY_OE\"],\n    categories=categories,\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=-1,\n    drop_input_cols=True\n)\n\n# Define the columns to encode\ncolumns_to_encode = [\n    \"INSURED_SEX\",\n    \"INSURED_OCCUPATION\",\n    \"INCIDENT_TYPE\",\n    \"AUTHORITIES_CONTACTED\",\n    \"POLICE_REPORT_AVAILABLE\"\n]\n# Create a OneHotEncoder instance\nOneHotEncoding = OneHotEncoder(\n    input_cols=columns_to_encode,\n    output_cols=[f\"{col}_encoded\" for col in columns_to_encode],\n    drop_input_cols=True,  # Keep original columns\n    handle_unknown='ignore'  # Ignore any unknown categories during transform\n)\n\n# Define the columns to scale\ncolumns_to_scale = [\n    'POLICY_LENGTH_MONTH',\n    'POLICY_DEDUCTABLE',\n    'POLICY_ANNUAL_PREMIUM',\n    'CLAIM_AMOUNT',\n    'POLICY_DURATION'\n]\n# Create the StandardScaler\nStandardScaling = StandardScaler(\n    input_cols=columns_to_scale,\n    output_cols=[f\"{col}_SCALED\" for col in columns_to_scale],\n    with_mean=True,\n    with_std=True,\n    drop_input_cols=True  # Keep original columns\n)\n\n# Determine the label column name\n# feature_columns = train_data.columns.remove('FRAUD_REPORTED_LONG')\nlabel_column = ['FRAUD_REPORTED']\noutput_column = ['PREDICTED_FRAUD']\n\n\n# # Initially, we can run this under the XGB Classifier model. However, you will notice that\n# # the model overfits on the training data and performs poorly on the test dataset\n# xgbmodel = XGBClassifier(\n#     random_state=1, \n#     #input_cols=feature_columns,    #here we are passing all columns so we have commented out. If you have specific columns set as features, you should specify them here\n#     label_cols=label_column,\n#     output_cols=output_column\n#     )\n\n\nxgb_grid_search = GridSearchCV(\n    estimator=XGBClassifier(),\n    param_grid={\n        \"n_estimators\":[10, 20, 30, 50, 100, 150, 200, 250, 300],\n        \"subsample\": [0.9, 0.5, 0.2],\n        \"max_depth\": range(2,10,1),\n        \"learning_rate\":[0.1, 0.06, 0.05, 0.03, 0.01, 0.005, 0.002, 0.001],\n    },\n    n_jobs = -1,\n    #input_cols=feature_columns,    #here we are passing all columns so we have commented out. \n                                    #If you have specific columns set as features, you should specify them here\n    label_cols=label_column,\n    output_cols=output_column,\n)\n\n# xgb_gs_fitted = xgb_grid_search.fit(train_data)\n\nmodel_pipeline = Pipeline(\n    steps=[\n        (\"Ordinal_encoding\",OrdinalEncoding),\n        (\"OneHotEncoding\",OneHotEncoding),\n        (\"standardscaler\",StandardScaling),\n        #(\"XGBClassifier\", xgbmodel)\n        (\"CV_XGBClassifier\", xgb_grid_search)\n    ]\n)"
  },
  {
   "cell_type": "code",
   "id": "27f9ad24-a05e-48ba-a56f-355c67ad804d",
   "metadata": {
    "language": "python",
    "name": "WH_Up"
   },
   "outputs": [],
   "source": "wh = str(session.get_current_warehouse()).strip('\"')\nprint(f\"Current warehouse: {wh}\")\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n\nsession.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = LARGE\").collect()\n\n#Give Snowflake a few seconds to change WH sizes\nimport time\ntime.sleep(5)\n\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "011ec4e2-ea34-45f0-b920-07ff4314b74c",
   "metadata": {
    "language": "python",
    "name": "Fit_Pipeline",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Fit the pipeline to the training data\nxgb_gs_fitted_training = model_pipeline.fit(train_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c80f2f05-512f-4869-9989-d609d3b1ffa3",
   "metadata": {
    "language": "python",
    "name": "WH_Down"
   },
   "outputs": [],
   "source": "wh = str(session.get_current_warehouse()).strip('\"')\nprint(f\"Current warehouse: {wh}\")\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n\nsession.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = SMALL\").collect()\n\n#Give Snowflake a few seconds to change WH sizes\nimport time\ntime.sleep(5)\n\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "979dfee7-3117-46eb-a8fe-0aee63e0a57c",
   "metadata": {
    "language": "python",
    "name": "MAPE_Graph",
    "collapsed": false
   },
   "outputs": [],
   "source": "gs_results = xgb_gs_fitted_training.to_sklearn().named_steps['CV_XGBClassifier'].cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\n\nsns.set_context(\"notebook\", font_scale=0.5)\nsns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\", height=3)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a87af6fc-21b0-4164-9fe0-b27ea41ad0af",
   "metadata": {
    "language": "python",
    "name": "xgb_gs_model_pred",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "xgb_gs_train = model_pipeline.predict(train_data)\nxgb_gs_predictions = model_pipeline.predict(test_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41394383-ce3e-414d-9271-29d0d7108f40",
   "metadata": {
    "language": "python",
    "name": "Acc_Score",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's start with the basic metric, Accuracy, which the number of correct predictions made divided by the total number of predictions made,\nACCURACY = accuracy_score(df=xgb_gs_predictions, y_true_col_names=label_column, y_pred_col_names=output_column)\nprint('Training Accuracy:', accuracy_score(df=xgb_gs_train, y_true_col_names=label_column, y_pred_col_names=output_column))\nprint(f'Test Acccuracy: {ACCURACY}')\n\n# RPC AUC is slightly perferred IMO. Anything above 50% or .5 is better than random guessing\nAUC = roc_auc_score(df=xgb_gs_predictions, y_true_col_names=label_column, y_score_col_names=output_column)\nprint('Training AUC:', roc_auc_score(df=xgb_gs_train, y_true_col_names=label_column, y_score_col_names=output_column))\nprint(f'Test AUC: {AUC}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa232d63-7925-4334-aa8a-5c2c0669d095",
   "metadata": {
    "language": "python",
    "name": "Helper_Model_Stat",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(\" Results from Grid Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\",model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\",model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\",model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].best_params_)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d3e7c-7b63-48fa-8ef6-a24474fe7d31",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "feature_ranking",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Plot feature importance\nfeat_importance = pd.DataFrame(model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].best_estimator_.feature_importances_,model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].feature_names_in_,columns=['FeatImportance'])\nfeat_importance.sort_values('FeatImportance').plot.barh(y='FeatImportance', figsize=(5,10))"
  },
  {
   "cell_type": "code",
   "id": "d9dfa318-66c6-44d2-974c-e445a3375e09",
   "metadata": {
    "language": "python",
    "name": "import_reg",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's now register the CV Classfier model into the model_registry\nReg = Registry(\n    session=session,\n    database_name=session.get_current_database(),\n    schema_name='data',\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53f436ba-1591-46da-8a6f-c44a32300e5b",
   "metadata": {
    "language": "python",
    "name": "helper_mod_ver",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# FUNCTION used to iterate the model version so we can automatically \n# create the next version number\nimport ast\nimport builtins  # Import the builtins module\n#from snowflake.snowpark import functions as F \n\ndef get_next_version(reg, model_name) -> str:\n    \"\"\"\n    Returns the next version of a model based on the existing versions in the registry.\n\n    Args:\n        reg: The registry object that provides access to the models.\n        model_name: The name of the model.\n\n    Returns:\n        str: The next version of the model in the format \"V_\".\n\n    Raises:\n        ValueError: If the version list for the model is empty or if the version format is invalid.\n    \"\"\"\n    models = reg.show_models()\n    if models.empty:\n        return \"V_1\"\n    elif model_name not in models[\"name\"].to_list():\n        return \"V_1\"\n    max_version_number = builtins.max(  \n        [\n            int(version.split(\"_\")[-1])\n            for version in ast.literal_eval(\n                models.loc[models[\"name\"] == model_name, \"versions\"].values[0]\n            )\n        ]\n    )\n    return f\"V_{max_version_number + 1}\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4c357-3aa0-4792-a17d-4c4f7b140d8c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "model_reg"
   },
   "outputs": [],
   "source": "model_name = 'XGB_GS_FRAUD_MODEL'\nmodel_version = get_next_version(Reg, model_name)\n\nmv = Reg.log_model(xgb_gs_fitted_training,\n    model_name=model_name,\n    version_name=model_version,\n    conda_dependencies=[\"snowflake-ml-python\"],\n    comment=\"Model trained using GridsearchCV in Snowpark to predict fraud claims\",\n    #metrics={\"Acc\": ACCURACY, \"AUC\": AUC}, # We can save our model metrics here\n    options= {\"relax_version\": False}\n)\n\nm = Reg.get_model(model_name)\nm.default = model_version"
  },
  {
   "cell_type": "code",
   "id": "b40684cd-b289-4afb-8547-f2d5552fd7a3",
   "metadata": {
    "language": "python",
    "name": "Helper_Model_Version",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# lets see the models we have in our registry\n\nReg.get_model(model_name).show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00cacecf-b7f3-4085-aee2-8d5c8a2283ae",
   "metadata": {
    "language": "python",
    "name": "save_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "st.dataframe(xgb_gs_predictions.limit(20))\n#xgb_gs_predictions.sample(n=200).write.save_as_table('SWT2024_DEMO_AUTO_INSURANCE.DATA.REFERENCE_DATA', mode = 'overwrite')",
   "execution_count": null
  }
 ]
}
