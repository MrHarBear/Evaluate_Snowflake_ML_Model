{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Title",
    "collapsed": false
   },
   "source": "#  **SWT 2024 Evaluating your Machine Learning Models in Snowflake**\n### Notebook 0 - Data Import\n---\n### What We'll Do:\nIn this notebook, we will use External Access Integration to load data Our data source is a GitHub repository, from which we'll fetch and directly store the data into our Snowflake account. No S3 buckets or local downloads are needed. \n\nPlease remember to add your External Access in the Notebook. You can access this by clicking on the &#8942; dropdown &#8594; External Access &#8594; enable `GITHUB_EXTERNAL_ACCESS_INTEGRATION`\n\nOur goal is to simplify the execution of this demo while showcasing the extensive capabilities of Snowflake!\n\n## THIS WILL ONLY WORK FOR FULL SNOWFLAKE ACCOUNT AND NOT A TRIAL ACCOUNT! "
  },
  {
   "cell_type": "code",
   "id": "ae6e18cb-968e-41e1-bb8c-9138b49031bd",
   "metadata": {
    "language": "python",
    "name": "Import_Packages",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nimport requests\nimport pandas as pd\nfrom snowflake.snowpark import DataFrame\nfrom io import StringIO\n\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "setting",
    "collapsed": false
   },
   "source": "snowflake_environment = session.sql('select current_user(), current_version()').collect()\nfrom snowflake.snowpark.version import VERSION\n\n# Current Environment Details\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(VERSION[0],VERSION[1],VERSION[2]))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bc6fd730-39a6-4e1f-9d70-5c410e746875",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false
   },
   "outputs": [],
   "source": "def fetch_dataset_from_github(url: str) -> 'DataFrame':\n    # Fetch the CSV data from the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        # Decode the content and read into a Pandas DataFrame\n        csv_data = response.content.decode('utf-8')\n        csv_file = StringIO(csv_data)\n        pandas_df = pd.read_csv(csv_file)\n        \n        # Convert Pandas DataFrame to Snowpark DataFrame\n        return session.create_dataframe(pandas_df)\n    else:\n        raise Exception(f\"Failed to fetch CSV: {response.status_code} - {response.text}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c28ee352-7885-4408-aa86-013937b580ea",
   "metadata": {
    "language": "python",
    "name": "Import_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Once an updated file is on the github dataset repo, it's enoguh to run this cell to reload the new datasets.\nfrom snowflake.snowpark.functions import col\n\n# If your data is saved somewhere else, change this to the right schema\ndata_schema = 'DATA'\n\n# Function to rename columns to uppercase\ndef rename_columns_to_uppercase(df):\n    # Generate a list of columns with uppercase names\n    new_columns = [col(c).alias(c.upper()) for c in df.columns]\n    # Select columns with new names\n    return df.select(*new_columns)\n\n# Base URL and list of files\nurl_base = 'https://github.com/MrHarBear/Evaluate_Snowflake_ML_Model/raw/main/datasets/'\nurl_files = [\n    'claim_data.csv',\n    'claim_data_new.csv',\n    'customer_data.csv'\n]\n\n# Loop through each URL\nfor url in url_files:\n    # Get Snowpark DataFrame from the URL\n    df = fetch_dataset_from_github(url_base + url)\n\n    # Extract table name from URL\n    table_name = url.split('/')[-1].replace('.csv', '').upper()\n\n    full_path = session.get_current_database().strip('\"') + '.' + data_schema + '.' + table_name\n    print(full_path)\n    # Drop the table if it exists\n    session.sql(f\"DROP TABLE IF EXISTS {full_path}\").collect()\n\n    # Convert column names to uppercase\n    df = rename_columns_to_uppercase(df)\n\n    # Create table and insert data from Snowpark DataFrame\n    df.write.save_as_table(full_path, mode='overwrite')\n\n    print(f\"Table {table_name} created and data loaded successfully.\")\n    session.table(full_path).show(5)",
   "execution_count": null
  }
 ]
}