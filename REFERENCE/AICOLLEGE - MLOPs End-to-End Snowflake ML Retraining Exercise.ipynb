{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ysosyamys6cdhxgdjf7p",
   "authorId": "167081822753",
   "authorName": "HCHEN",
   "authorEmail": "harley.chen@snowflake.com",
   "sessionId": "9613fb39-5d84-4083-adbf-a5979f21e86b",
   "lastEditTime": 1751971921063
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "End2EndSnowflakeMLFlow"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Define image in a stage and read the file\n",
    "image=session.file.get_stream('@aicollege.public.setup/SnowflakeML.jpg' , decompress=False).read() \n",
    "\n",
    "# Display the image\n",
    "st.image(image, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "collapsed": false,
    "name": "NotebookOverview"
   },
   "source": [
    "### ‚ùÑÔ∏è End-to-End ML Retraining with Snowflake and Open-Source Models ‚ùÑÔ∏è\n",
    "\n",
    "This section walks through the full retraining pipeline using **Snowflake-native data**, **open-source ML libraries**, and the **Snowflake Model Registry** to version, monitor, and explain updated model behavior.\n",
    "\n",
    "Before running this notebook, make sure the enriched dataset has been loaded into the `NEWTRAININGDATA` table. This table combines historical and recent mortgage data to improve generalization and reduce drift.\n",
    "\n",
    "In this retraining pipeline, we will:\n",
    "\n",
    "- Load and explore the enriched data from the `NEWTRAININGDATA` table  \n",
    "- Preprocess inputs and engineer features  \n",
    "- Register features into the **Snowflake Feature Store**\n",
    "- Retrain a model using **open-source ML frameworks** like `xgboost` or `scikit-learn`\n",
    "- Register the updated model as **Version 2** in the **Snowflake Model Registry**\n",
    "- Set up **ML Monitors** to compare V1 vs. V2 on performance drift\n",
    "- Use **Snowflake ML Explainability** (`EXPLAIN`) to generate SHAP-style insights\n",
    "- Promote the best-performing model version to production using **aliases or version control**\n",
    "- Run **dynamic batch scoring** in Python **using the production alias**\n",
    "\n",
    "> ‚úÖ This notebook assumes your original model (Version 1) is already registered as `COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL` in the Snowflake Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d00a29-4e7d-4080-9d30-f0f4308089b9",
   "metadata": {
    "language": "sql",
    "name": "NotebookReadinessCheck"
   },
   "outputs": [],
   "source": [
    "-- Validate query COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL version V1 is registered in Snowflake Model Registry\n",
    "SHOW VERSIONS IN MODEL COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c0562-465c-4009-9d48-4dc7c2c5f34b",
   "metadata": {
    "collapsed": false,
    "name": "LoadRetrainingData"
   },
   "source": [
    "### üì• Load and Inspect the Enriched Training Dataset\n",
    "\n",
    "In this step, we load the `NEWTRAININGDATA` table, which contains a curated dataset designed to support retraining on more current and representative patterns. It includes:\n",
    "\n",
    "- Original training data capturing historical trends\n",
    "- Recent labeled examples from production (e.g., batch inference results with ground truth)\n",
    "- Optionally, synthetic records to match the distribution of current input patterns\n",
    "\n",
    "Rather than using a simple `SELECT * ... LIMIT`, we‚Äôll preview the data using a query that returns a **sample of rows per `LOAN_PURPOSE_NAME`**. This approach gives a more balanced snapshot of the dataset across different loan purposes, which helps validate readiness for downstream preprocessing and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "ViewRetrainingData"
   },
   "outputs": [],
   "source": [
    "SELECT * EXCLUDE (rn)\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY LOAN_TYPE_NAME ORDER BY LOAN_ID) AS rn\n",
    "    FROM AICOLLEGE.PUBLIC.NEWTRAININGDATA\n",
    ")\n",
    "WHERE rn <= 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728c10b-1e7e-43b9-94b5-12128a02779b",
   "metadata": {
    "collapsed": false,
    "name": "Preprocessing"
   },
   "source": [
    "### üßπ Data Cleaning & Sanitization\n",
    "\n",
    "Before we apply One-Hot Encoding or register features to the Snowflake Feature Store, we create a cleaned and standardized version of the training data called `NEWTRAININGDATA_CLEANED`. This ensures downstream steps operate on consistent, ML-friendly inputs.\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- **Map known loan-type values**  \n",
    "  Standardize strings for model compatibility:  \n",
    "  - `FSA/RHS-guaranteed` ‚Üí `FSA_RHS`  \n",
    "  - `FHA-insured` ‚Üí `FHA`  \n",
    "  - `VA-guaranteed` ‚Üí `VA`  \n",
    "\n",
    "- **Normalize key categorical fields**  \n",
    "  - `LOAN_PURPOSE_NAME`: Replace hyphens, spaces, and periods with underscores (e.g., `home-improvement` ‚Üí `home_improvement`)  \n",
    "  - `COUNTY_NAME`: Remove hyphens, underscores, and periods entirely (e.g., `St. Lawrence County` ‚Üí `St Lawrence County`)  \n",
    "\n",
    "- **Preserve temporal and ID metadata**  \n",
    "  Retain `WEEK_START_DATE`, `WEEK`, `LOAN_ID`, and `TS` fields for observability and feature tracking.\n",
    "\n",
    "- **Ensure correct numeric types**  \n",
    "  Explicitly cast `APPLICANT_INCOME_000S` and `LOAN_AMOUNT_000S` as `FLOAT` to prevent type issues during feature engineering or training.\n",
    "\n",
    "The resulting `NEWTRAININGDATA_CLEANED` table provides a consistent and sanitized foundation for one-hot encoding, feature store registration, and model retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3e98c-02fb-423b-b716-adf0601f1cf1",
   "metadata": {
    "language": "sql",
    "name": "PreprocessingCode"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE AICOLLEGE.PUBLIC.NEWTRAININGDATA_CLEANED AS\n",
    "SELECT\n",
    "  -- normalize loan type\n",
    "  REPLACE(\n",
    "    REPLACE(\n",
    "      REPLACE(LOAN_TYPE_NAME,\n",
    "        'FSA/RHS-guaranteed', 'FSA_RHS'),\n",
    "        'FHA-insured', 'FHA'),\n",
    "        'VA-guaranteed', 'VA'\n",
    "  ) AS LOAN_TYPE_NAME,\n",
    "\n",
    "  -- normalize loan purpose\n",
    "  REPLACE(\n",
    "    REPLACE(\n",
    "      REPLACE(LOAN_PURPOSE_NAME,\n",
    "        '-', '_'),\n",
    "        ' ', '_'),\n",
    "        '.', '_'\n",
    "  ) AS LOAN_PURPOSE_NAME,\n",
    "\n",
    "  -- normalize county\n",
    "  REPLACE(\n",
    "    REPLACE(\n",
    "      REPLACE(COUNTY_NAME,\n",
    "        '-', ''),   -- remove hyphens\n",
    "        '_', ''),   -- remove underscores\n",
    "        '.', ''     -- remove periods\n",
    "  ) AS COUNTY_NAME,\n",
    "\n",
    "  WEEK_START_DATE,\n",
    "  WEEK,\n",
    "  LOAN_ID,\n",
    "  TS,\n",
    "  APPLICANT_INCOME_000S,\n",
    "  LOAN_AMOUNT_000S,\n",
    "  MORTGAGERESPONSE\n",
    "FROM AICOLLEGE.PUBLIC.NEWTRAININGDATA;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e614a-77e6-410c-853c-435080de6a39",
   "metadata": {
    "collapsed": false,
    "name": "FeatureEngineeringFeatureStore"
   },
   "source": [
    "### üß™ Feature Engineering & Feature Store Setup\n",
    "\n",
    "In this phase, we define and register a consistent set of engineered features using the cleaned `NEWTRAININGDATA_CLEANED` dataset as input. These features are transformed using **Snowflake ML preprocessing** and stored in the **Feature Store** for reuse across training and scoring workflows.\n",
    "\n",
    "The **Snowflake Feature Store** enables:\n",
    "- ‚úÖ Centralized tracking of feature definitions, lineage, and versions  \n",
    "- ‚ôªÔ∏è Reuse of production-ready features across both training and inference pipelines  \n",
    "- üîç Transparent, reproducible, and maintainable ML workflows\n",
    "\n",
    "---\n",
    "\n",
    "### What we‚Äôll do:\n",
    "1. Identify categorical features (`LOAN_TYPE_NAME`, `LOAN_PURPOSE_NAME`, `COUNTY_NAME`) and apply **One-Hot Encoding (OHE)** using `OneHotEncoder` from *snowflake.ml*.  \n",
    "2. Keep the original raw columns for **transparency and lineage**.  \n",
    "3. Fill in any missing values to ensure clean inputs.  \n",
    "4. Write the transformed dataset to a persistent table `NEWTRAININGDATA_FINAL`.  \n",
    "5. Optionally register engineered features in the Feature Store for consistent batch inference and monitoring.\n",
    "\n",
    "> üß± Separating raw definitions from model-specific transformations gives us:\n",
    "> - ü™Ñ Reusable and modular feature logic  \n",
    "> - üîÅ Flexibility to experiment with alternative encoding strategies  \n",
    "> - üß™ Reliable inputs for training, production scoring, and explainability\n",
    "\n",
    "> ‚ö†Ô∏è **NumPy 2.0 Note**:  \n",
    "> Snowflake ML expects the `np.float_` alias, which was removed in NumPy ‚â• 2.0. Add this shim before importing `OneHotEncoder`:\n",
    "> ```python\n",
    "> import numpy as np  \n",
    "> if not hasattr(np, \"float_\"):  \n",
    ">     np.float_ = np.float64  \n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3407fc-ac37-483d-b29c-0d815fa3a91c",
   "metadata": {
    "language": "python",
    "name": "FeatureEngineeringCode"
   },
   "outputs": [],
   "source": "# --- Feature Engineering with OneHotEncoder in Snowflake ML ---\nimport numpy as np\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import StringType, IntegerType, FloatType, DoubleType, DecimalType\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\n\n# Temporary patch for NumPy >= 2.0\nif not hasattr(np, 'float_'):\n    np.float_ = np.float64\n\n# Load cleaned training data\ndf_retrain = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_CLEANED\")\n\n# Identify categorical columns (exclude timestamp and label)\ncategorical_cols = [\n    f.name\n    for f in df_retrain.schema\n    if isinstance(f.datatype, StringType)\n       and f.name not in (\"TS\", \"MORTGAGERESPONSE\")\n]\n\n# Create output column names with _OHE suffix\nencoded_cols = [f\"{c}_OHE\" for c in categorical_cols]\n\n# Instantiate OneHotEncoder\nohe = OneHotEncoder(        # --> Use Snowflake's OneHotEncoder function\n    input_cols=categorical_cols,\n    output_cols=encoded_cols,\n    drop_input_cols=False\n)\n\n# Apply encoder\ndf_encoded = ohe.fit(df_retrain).transform(df_retrain)\n\n# Identify numeric columns only for safe fillna\nnumeric_cols = [\n    f.name for f in df_encoded.schema\n    if isinstance(f.datatype, (IntegerType, FloatType, DoubleType, DecimalType))\n]\n\n# Apply fillna only to numeric columns\ndf_encoded = df_encoded.fillna({col_name: 0 for col_name in numeric_cols})\n\n# Save encoded data to Snowflake table\ndf_encoded.write.mode(\"overwrite\").save_as_table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\")  # --> Save as table AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\n\n# Reload as a persisted DataFrame\ndf_encoded_persisted = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\")"
  },
  {
   "cell_type": "markdown",
   "id": "27dc2167-0003-49e0-92fc-2d9b59c033be",
   "metadata": {
    "collapsed": false,
    "name": "FeatureStore"
   },
   "source": [
    "### üóÑÔ∏è Register Loan Features with the Feature Store\n",
    "\n",
    "Now that we‚Äôve packaged our cleaned, one-hot encoded data (`NEWTRAININGDATA_FINAL`),  \n",
    "we‚Äôll register exactly the features our model needs in the **Snowflake Feature Store**.\n",
    "\n",
    "Why this matters:\n",
    "- üéØ **Single source of truth** - the same features feed training, inference, and monitoring.\n",
    "- üîç **Time‚Äëaware joins & lineage** ‚Äì every feature is versioned and timestamped.\n",
    "- üîÑ **Model‚Äëagnostic** ‚Äì any future model can reuse the view without new ETL.\n",
    "\n",
    "In this step, we will:\n",
    "1. Define a `LOAN_ENTITY` on the `LOAN_ID` primary key  \n",
    "2. Select only the OHE columns (those containing `\"_OHE_\"`) **plus** the numeric features  \n",
    "   - `APPLICANT_INCOME_000S`  \n",
    "   - `LOAN_AMOUNT_000S`  \n",
    "   - `WEEK`  \n",
    "3. **Exclude** any leakage columns (`MORTGAGERESPONSE`, `TS`, etc.)  \n",
    "4. Register a Feature View (e.g. `LOAN_FEATURES` or `OHE_FEATURE_VIEW`)  \n",
    "   using `WEEK_START_DATE` as the timestamp for time-travel joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5f183-b940-44b9-876f-477059ed84a5",
   "metadata": {
    "language": "python",
    "name": "FeatureStoreCode"
   },
   "outputs": [],
   "source": "# --- Register One-Hot Encoded Features with Snowflake Feature Store \nimport warnings\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\nfrom snowflake.snowpark.functions import col\n\n# 1) Read your encoded table\ndf_encoded = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\")\n\n# 2) Identify raw feature columns\nnumeric_cols = [\"APPLICANT_INCOME_000S\", \"LOAN_AMOUNT_000S\", \"WEEK\"]\nraw_ohe_cols = [c for c in df_encoded.columns if \"_OHE_\" in c]\n\n# 3) Build a rename map to strip quotes & replace spaces/dots with underscores\ndef sanitize(name):\n    return name.strip('\"').replace(\" \", \"_\").replace(\".\", \"_\")\n\nrename_map = {c: sanitize(c) for c in [\"LOAN_ID\", \"WEEK_START_DATE\"] + raw_ohe_cols + numeric_cols}\n\n# 4) Select & rename in one go\ndf_features = df_encoded.select([\n    col(old).alias(new)\n    for old, new in rename_map.items()\n])\n\n# 5) Initialize Feature Store\nfs = FeatureStore(  # --> Use Snowflake's Feature Store (FeatureStore)\n    session=session,\n    database=\"AICOLLEGE\",\n    name=\"PUBLIC\",\n    default_warehouse=\"AICOLLEGE\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\n# 6) Register entity\nloan_entity = Entity(name=\"LOAN_ENTITY\", join_keys=[\"LOAN_ID\"])  # --> Save as LOAN_ENTITY as the Feature Store Entity\nfs.register_entity(loan_entity)\n\n# 7) Register Feature View\nfv = FeatureView(  # --> Use Snowflake's Feature View (FeatureView)\n    name=\"LOAN_FEATURES_OHE\",  # --> Save as LOAN_FEATURES_OHE Feature View\n    entities=[loan_entity],\n    feature_df=df_features,\n    timestamp_col=\"WEEK_START_DATE\",  # --> Ensure you include your TIMESTAMP column with TIMESTAMP_NTZ format (WEEK_START_DATE)\n    refresh_freq=\"1 day\"  # --> Ensure your Snowflake Feature Store (dynamic table) has a valid refresh frequency value\n)\nfs.register_feature_view(fv, version=\"1\", overwrite=True)\nprint(\"‚úÖ Feature View registered with\", len(rename_map), \"features:\", list(rename_map.values()))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a25584-2d84-4927-841c-2333a2869dbf",
   "metadata": {
    "language": "sql",
    "name": "ViewFeatureView"
   },
   "outputs": [],
   "source": [
    "-- List dynamic tables created by the Feature Store for OHE_FEATURE_VIEW.\n",
    "SHOW TABLES LIKE 'LOAN_FEATUREs_OHE%';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84828c2a-b30d-49bc-bfc3-9c4cceac0850",
   "metadata": {
    "collapsed": false,
    "name": "RetrainModel"
   },
   "source": [
    "### üîÑ Retrain and Compare ML Models (Using OSS Libraries)\n",
    "\n",
    "Now that our **one-hot encoded features** are saved in the `NEWTRAININGDATA_FINAL` table, we‚Äôll retrain multiple candidate models using the enriched training dataset.\n",
    "\n",
    "This step includes:\n",
    "\n",
    "- **Loading** the feature-engineered data from Snowflake  \n",
    "- Converting it to a **Pandas DataFrame** for modeling  \n",
    "- Splitting into **train (80% ‚âà 4,240 samples)** and **test (20% ‚âà 1,060 samples)** sets  \n",
    "- **Training** and **evaluating** three OSS-based models:  \n",
    "  - XGBoost  \n",
    "  - Random Forest  \n",
    "  - Logistic Regression  \n",
    "- Comparing their performance using **test set accuracy**\n",
    "\n",
    "### ‚ùó Why use Pandas?\n",
    "\n",
    "While your data lives in Snowflake, OSS libraries like `scikit-learn` and `xgboost` require **in-memory data**. They do **not support Snowpark DataFrames** directly.  \n",
    "For that reason, we convert Snowflake data to a Pandas DataFrame before training models locally in the notebook.\n",
    "\n",
    "This approach:\n",
    "- ‚úÖ Aligns with Snowflake‚Äôs guidance to use OSS libraries for modeling  \n",
    "- ‚öôÔ∏è Enables flexible development and model experimentation  \n",
    "- üîÅ Keeps the pipeline compatible with Snowflake Model Registry and Batch Inference later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507113a-0c27-48af-8baa-f1063ed7c661",
   "metadata": {
    "language": "python",
    "name": "ModelSetup"
   },
   "outputs": [],
   "source": [
    "# --- Model setup using only 'LOAN_PURPOSE_NAME_OHE' to reduce dimensionality ---\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark.types import FloatType\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load OHE features ‚Äî only keep what you need\n",
    "df_features_all = session.table(\"AICOLLEGE.PUBLIC.LOAN_FEATURES_OHE$1\")\n",
    "\n",
    "# 2. Filter columns\n",
    "ohe_cols_to_keep = [c for c in df_features_all.columns if c.startswith(\"LOAN_PURPOSE_NAME_OHE\")]\n",
    "selected_cols = [\"LOAN_ID\", \"WEEK_START_DATE\", \"APPLICANT_INCOME_000S\", \"LOAN_AMOUNT_000S\"] + ohe_cols_to_keep\n",
    "df_features = df_features_all.select([col(c) for c in selected_cols])\n",
    "\n",
    "# 3. Load labels\n",
    "df_labels = session.table(\"AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\") \\\n",
    "                   .select(\"LOAN_ID\", \"WEEK_START_DATE\", \"MORTGAGERESPONSE\")\n",
    "\n",
    "# 4. Join + cast numeric columns in Snowpark\n",
    "df_joined = (\n",
    "    df_features.join(df_labels, on=[\"LOAN_ID\", \"WEEK_START_DATE\"])\n",
    "    .with_column(\"APPLICANT_INCOME_000S\", col(\"APPLICANT_INCOME_000S\").cast(FloatType()))\n",
    "    .with_column(\"LOAN_AMOUNT_000S\", col(\"LOAN_AMOUNT_000S\").cast(FloatType()))\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "# 5. Clean column names and drop missing\n",
    "df_joined.columns = df_joined.columns.str.strip('\"').str.replace(\" \", \"_\")\n",
    "df_joined = df_joined.dropna()\n",
    "\n",
    "# 6. Define X, y and keys\n",
    "label = \"MORTGAGERESPONSE\"  # --> Which column is the target variable?\n",
    "feature_cols = ohe_cols_to_keep + [\"APPLICANT_INCOME_000S\", \"LOAN_AMOUNT_000S\"]\n",
    "X = df_joined[feature_cols]\n",
    "y = df_joined[label].astype(int)\n",
    "id_vec    = df_joined[\"LOAN_ID\"].astype(int)\n",
    "date_vec  = pd.to_datetime(df_joined[\"WEEK_START_DATE\"])\n",
    "\n",
    "# 7. Train/test split for all 4 arrays\n",
    "X_train, X_test, y_train, y_test, id_train, id_test, date_train, date_test = train_test_split(\n",
    "    X, y, id_vec, date_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 8. Report\n",
    "print(f\"‚úÖ Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df_joined)*100:.1f}%)\")\n",
    "print(f\"‚úÖ Test  set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df_joined)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a78f28-8831-41bc-985d-ac2f96326fa8",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "TrainModels"
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Scale your data\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(\n    scaler.fit_transform(X_train),\n    columns=X_train.columns,\n    index=X_train.index\n)\nX_test_scaled = pd.DataFrame(\n    scaler.fit_transform(X_test),\n    columns=X_test.columns,\n    index=X_test.index\n)\n\n# Use a faster solver for large datasets\nlogreg_model = LogisticRegression(max_iter=1000, solver=\"saga\")\nrf_model     = RandomForestClassifier(n_estimators=50)  # reduce estimators for speed\nxgb_model    = XGBClassifier(eval_metric=\"logloss\", n_estimators=50)\n\n# Train the models\nlogreg_model.fit(X_train_scaled, y_train)\nrf_model.fit(X_train_scaled, y_train)\nxgb_model.fit(X_train_scaled, y_train)\n\n# Evaluate\nprint(\"üîç Retrain Model Accuracy by candidate model:\")\nprint(f\"XGBoost:             {accuracy_score(y_test, xgb_model.predict(X_test_scaled)) * 100:.2f}%\")\nprint(f\"Random Forest:       {accuracy_score(y_test, rf_model.predict(X_test_scaled)) * 100:.2f}%\")\nprint(f\"Logistic Regression: {accuracy_score(y_test, logreg_model.predict(X_test_scaled)) * 100:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5861b-2fd3-457a-a000-e3a3309acbc3",
   "metadata": {
    "language": "python",
    "name": "PersistV2Predictions"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, row_number, sql_expr\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "# Build the prediction DataFrame as pandas\n",
    "df_raw = pd.DataFrame({\n",
    "    \"LOAN_ID\":            id_test.values,\n",
    "    \"WEEK_START_DATE\":    date_test.values,\n",
    "    \"PREDICTED_RESPONSE\": logreg_model.predict(X_test_scaled).astype(int),\n",
    "    \"PREDICTED_SCORE\":    logreg_model.predict_proba(X_test_scaled)[:, 1],\n",
    "    \"MORTGAGERESPONSE\":   y_test.values.astype(int),\n",
    "})\n",
    "\n",
    "# Convert to Snowpark DataFrame without passing schema\n",
    "df_snowpark = session.create_dataframe(df_raw)\n",
    "\n",
    "# Define a Snowpark window by WEEK_START_DATE with random sort\n",
    "window_spec = Window.partition_by(\"WEEK_START_DATE\").order_by(sql_expr('RANDOM()'))\n",
    "\n",
    "# Assign row number for sampling\n",
    "df_with_row_number = df_snowpark.with_column(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to first 200 rows per WEEK_START_DATE\n",
    "df_sampled = df_with_row_number.filter(col(\"row_num\") <= 200).drop(\"row_num\")\n",
    "\n",
    "# Persist to Snowflake\n",
    "df_sampled.write.mode(\"overwrite\").save_as_table(\"AICOLLEGE.PUBLIC.V2_RAW_PREDICTIONS\")\n",
    "\n",
    "print(\"‚úÖ V2 prediction sets persisted for Model Monitor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a0787-e9dd-4150-8982-170c4086ee20",
   "metadata": {
    "language": "sql",
    "name": "CreateV2PredictionsView"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE VIEW AICOLLEGE.PUBLIC.V2_PREDICTIONS AS\n",
    "WITH b_deduped AS (\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY LOAN_ID, WEEK_START_DATE ORDER BY TS DESC) AS rn\n",
    "    FROM AICOLLEGE.PUBLIC.NEWTRAININGDATA_FINAL\n",
    "  )\n",
    "  WHERE rn = 1\n",
    ")\n",
    "SELECT\n",
    "  b.WEEK_START_DATE,\n",
    "  b.WEEK,\n",
    "  b.LOAN_ID,\n",
    "  b.TS,\n",
    "  b.LOAN_TYPE_NAME,\n",
    "  b.LOAN_PURPOSE_NAME,\n",
    "  CAST(b.APPLICANT_INCOME_000S AS FLOAT) AS APPLICANT_INCOME_000S,\n",
    "  b.LOAN_AMOUNT_000S,\n",
    "  b.COUNTY_NAME,\n",
    "  r.MORTGAGERESPONSE,\n",
    "  r.PREDICTED_RESPONSE,\n",
    "  r.PREDICTED_SCORE\n",
    "FROM AICOLLEGE.PUBLIC.V2_RAW_PREDICTIONS AS r\n",
    "JOIN b_deduped AS b\n",
    "  USING (LOAN_ID, WEEK_START_DATE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24910d-b8f1-464a-9981-bf0225a640c4",
   "metadata": {
    "collapsed": false,
    "name": "RegisterModel"
   },
   "source": [
    "### üì¶ Register Updated Model (Version 2)\n",
    "\n",
    "We‚Äôve retrained three OSS models (**XGBoost**, **Random Forest**, **Logistic Regression**) using the enriched `NEWTRAININGDATA` table.  \n",
    "**Logistic Regression** achieved the best accuracy (~68%), so we will register this model in the **Snowflake Model Registry**.\n",
    "\n",
    "> ‚úÖ **Why register Logistic Regression?**\n",
    "> \n",
    "> - Best performing model on updated data\n",
    "> - Lightweight, interpretable, and easy to monitor\n",
    "> - Aligns with OSS-first ML development best practices\n",
    "\n",
    "We will register the model with:\n",
    "- Full model artifact\n",
    "- Version control for lifecycle management\n",
    "- Metadata including training accuracy\n",
    "\n",
    "> ‚ö†Ô∏è **Note:**  \n",
    "> If you see a long OCSP-related warning like `fail-open to connect` or `Could not fetch OCSP response`, you can safely ignore it ‚Äî the model will still register successfully. This warning is related to certificate checks and does **not** affect registration or functionality.\n",
    "\n",
    "\n",
    "> ‚ÑπÔ∏è **Note:**  \n",
    "> We are **not yet assigning a production alias** (like `production` or `current_best`) ‚Äî aliasing will happen after observability evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216bb63-3dea-4ec9-8778-b55aa6fb1634",
   "metadata": {
    "language": "python",
    "name": "RegisterRetrainedModel"
   },
   "outputs": [],
   "source": "# --- ‚úÖ Register OSS Logistic Regression as V2 of Existing Model ---\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.model import type_hints\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nimport logging\n\nsession  = get_active_session()\nregistry = Registry(session=session)\n\n# Suppress verbose Snowflake connector logs\nlogging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\nlogging.getLogger(\"snowflake.ml\").setLevel(logging.WARNING)\nlogging.basicConfig(level=logging.WARNING)\n\n# Evaluate the Logistic Regression model\ny_pred = xgb_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred, output_dict=True)\nconf_matrix = confusion_matrix(y_test, y_pred).tolist()\n\n# Sample input for registration\nsample_input_data = X_train.copy().sample(n=5, random_state=42)\n\n# Log as V2 of the original registered model\nmodel_version = registry.log_model(\n    model=xgb_model,\n    model_name=\"COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\",  # >-- Provide required MLOPs HOL model name\n    version_name=\"V2\",                               # >-- Provide model version name (not V1)\n    sample_input_data=sample_input_data,\n    conda_dependencies=[\"scikit-learn\"],\n    comment=\"\"\"Version 2: Logistic Regression model trained using OSS inside Snowflake.\nThis version supports Snowflake ML Observability and Explainability.\"\"\",\n    metrics={\n        \"accuracy\": accuracy,\n        \"f1_score\": f1,\n        \"classification_report\": report,\n        \"confusion_matrix\": conf_matrix\n    },\n    task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n    target_platforms=[\"WAREHOUSE\"],  # >-- Ensures model is warehouse-compatible\n    options={\n        \"enable_explainability\": True,\n        \"method_options\": {\n            \"predict\": {\"case_sensitive\": True}\n        }\n    }\n)\n\nprint(\"‚úÖ Logistic Regression model registered as Version 2:\", model_version.version_name)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2af27-4e00-488b-8c68-1cbe891aa204",
   "metadata": {
    "language": "sql",
    "name": "DORA_SEAI53"
   },
   "outputs": [],
   "source": [
    "-- SEAI53: Validate model version V2 was registered in Snowflake Model Registry\n",
    "SELECT util_db.public.se_grader(\n",
    "  'SEAI53',\n",
    "  (actual >= 1),\n",
    "  actual,\n",
    "  1,\n",
    "  '‚úÖ Model Version V2 successfully registered!'\n",
    ") AS graded_results\n",
    "FROM (\n",
    "  SELECT COUNT(*) AS actual\n",
    "  FROM AICOLLEGE.INFORMATION_SCHEMA.MODEL_VERSIONS\n",
    "  WHERE MODEL_NAME = 'COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL'\n",
    "    AND MODEL_VERSION_NAME = 'V2'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b988d5-83f6-4c31-975c-f6434ae7bf4d",
   "metadata": {
    "collapsed": false,
    "name": "ModelMonitor"
   },
   "source": [
    "### üìà Enable Model Monitoring with Snowflake ML Observability\n",
    "\n",
    "Now that both **Version 1** and **Version 2** of our **XGBoost mortgage response model** are registered, we‚Äôll use **Snowflake ML Observability** to track and compare their real-world performance over time.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why Model Monitoring Matters\n",
    "- üìä Track critical metrics like **accuracy**, **precision**, **recall**, and **F1 score**\n",
    "- üåÄ Detect **prediction drift** and **feature drift** using recent inference batches\n",
    "- üß† Establish a **baseline** for expected model behavior\n",
    "- üì∫ Monitor performance trends using **Snowsight‚Äôs built-in dashboards** ‚Äî no external setup required\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Why We‚Äôre Re-Creating Monitors for V1 and V2\n",
    "\n",
    "We previously removed the original monitor for **Version 1**, but in order to use Snowsight‚Äôs **Compare Models** feature:\n",
    "\n",
    "- Each model version must have its **own active monitor**\n",
    "- We‚Äôll now re-create both monitors to support side-by-side comparison between:\n",
    "  - ‚úÖ **Version 1** (original XGBoost model)\n",
    "  - ‚úÖ **Version 2** (retrained XGBoost with updated feature store data)\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ Why Use `ALL_PREDICTIONS_WITH_GROUND_TRUTH`\n",
    "\n",
    "To ensure monitoring reflects **real-world inference behavior**, we‚Äôll base both monitors on the `ALL_PREDICTIONS_WITH_GROUND_TRUTH` table:\n",
    "- ‚úÖ Contains **actual predictions** made by deployed model versions\n",
    "- ‚úÖ Includes **ground truth labels** for evaluation\n",
    "- ‚úÖ Avoids leakage from training data or incomplete inference rows\n",
    "\n",
    "---\n",
    "\n",
    "This setup ensures transparent monitoring and lets us clearly assess whether Version 2 offers **consistent improvements** over Version 1 ‚Äî not just in test performance, but in **ongoing model behavior in production**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e0b04a5-ceb0-435e-9182-b74870757b41",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "\n\nALTER TABLE AICOLLEGE.PUBLIC.ALL_PREDICTIONS_WITH_GROUND_TRUTH\nRENAME COLUMN LOAN_AMOUNT_O80S TO LOAN_AMOUNT_000S;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27f50e3e-a50d-4825-96a6-e2e929af9c11",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "SELECT * FROM BASELINE_PREDICTIONS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40707fa2-3532-4549-9475-a307fea94266",
   "metadata": {
    "language": "sql",
    "name": "CreateModelMonitorV1"
   },
   "outputs": [],
   "source": "-- Recreate ML Observability Model Monitor for XGBoost (V1) model.\nCREATE OR REPLACE MODEL MONITOR MORTGAGE_MODEL_MONITOR_V1        -- Create for model version V1\nWITH \n  MODEL = AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL,    -- Provide required MLOPs HOL model name\n  VERSION = 'V1',\n  FUNCTION = 'predict',\n  SOURCE = AICOLLEGE.PUBLIC.ALL_PREDICTIONS_WITH_GROUND_TRUTH,   -- Full batch inference table\n  BASELINE = AICOLLEGE.PUBLIC.BASELINE_PREDICTIONS,              -- Baseline model or training sample\n  WAREHOUSE = AICOLLEGE,\n  REFRESH_INTERVAL = '365 DAY',\n  AGGREGATION_WINDOW = '7 DAYS',\n  TIMESTAMP_COLUMN = WEEK_START_DATE,\n  ID_COLUMNS = ('LOAN_ID'),\n  PREDICTION_CLASS_COLUMNS = ('PREDICTED_RESPONSE'),\n  ACTUAL_CLASS_COLUMNS = ('MORTGAGERESPONSE'),\n  PREDICTION_SCORE_COLUMNS = ('PREDICTED_SCORE');"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0ca2a-f756-4c8c-b071-a220231c5e8c",
   "metadata": {
    "language": "sql",
    "name": "CreateModelMonitorV2"
   },
   "outputs": [],
   "source": [
    "-- Create new Model Monitor for the retrained XGBoost (V2) model.\n",
    "CREATE OR REPLACE MODEL MONITOR MORTGAGE_MODEL_MONITOR_V2\n",
    "WITH \n",
    "  MODEL = AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL,\n",
    "  VERSION = 'V2',\n",
    "  FUNCTION = 'predict',\n",
    "  SOURCE = AICOLLEGE.PUBLIC.V2_PREDICTIONS,                -- <-- V2 training predictions\n",
    "  BASELINE = AICOLLEGE.PUBLIC.BASELINE_PREDICTIONS,        -- <-- Baseline model\n",
    "  WAREHOUSE = AICOLLEGE,\n",
    "  REFRESH_INTERVAL = '365 DAY',\n",
    "  AGGREGATION_WINDOW = '7 DAYS',\n",
    "  TIMESTAMP_COLUMN = WEEK_START_DATE,\n",
    "  ID_COLUMNS = ('LOAN_ID'),\n",
    "  PREDICTION_CLASS_COLUMNS = ('PREDICTED_RESPONSE'),\n",
    "  ACTUAL_CLASS_COLUMNS = ('MORTGAGERESPONSE'),\n",
    "  PREDICTION_SCORE_COLUMNS = ('PREDICTED_SCORE');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69962a-3cc1-4ecd-ad71-a289ecb3da60",
   "metadata": {
    "collapsed": false,
    "name": "ExploreModelMonitorDashboards"
   },
   "source": [
    "### üîç Compare Model Versions in Snowsight\n",
    "\n",
    "Now that monitors are set up for **Version 1** and **Version 2**, use Snowsight‚Äôs **Compare** feature to view them side-by-side.\n",
    "\n",
    "1. In Snowsight, go to **AI & ML > Models**\n",
    "2. Select **`COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL`**\n",
    "3. Open either **V1** or **V2**\n",
    "4. Under **Monitors**, select **`MORTGAGE_MODEL_MONITOR`**\n",
    "5. Click the **‚ÄúCompare‚Äù** toggle at the top of the monitor dashboard\n",
    "6. Select both **V1** and **V2**\n",
    "7. Set the date range to show **March 1, 2025 to July 20, 2025** of activity\n",
    "\n",
    "Even though both versions use the same XGBoost model architecture, this comparison view helps validate consistency and monitor subtle trends across model versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad711a-4522-4983-8ad6-9de400e009f1",
   "metadata": {
    "collapsed": false,
    "name": "MLExplainability"
   },
   "source": [
    "### üîç Enable Model Explainability with Snowflake `EXPLAIN`\n",
    "\n",
    "With our model now deployed and generating predictions, we can use **Snowflake‚Äôs built-in `EXPLAIN` function** to understand how input features contribute to model outputs ‚Äî directly in Snowflake, without requiring any external libraries.\n",
    "\n",
    "By calling the `EXPLAIN` function on a registered model version, we retrieve **feature contribution scores** (inspired by SHAP) that estimate how much each input influenced the model‚Äôs decision.\n",
    "\n",
    "‚úÖ Supported for models trained using **Snowflake ML** (e.g., XGBoost, LightGBM, Scikit-learn)  \n",
    "‚úÖ Provides per-feature contribution scores for individual predictions  \n",
    "‚úÖ Seamlessly integrated into **Python or SQL** workflows\n",
    "\n",
    "In this section, we compute these contributions for a representative sample of our data and visualize the results for both **Version 1** and **Version 2** of our model ‚Äî helping compare **which inputs drove predictions** across versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cda0fb-26ca-4625-be29-7d490ccefefe",
   "metadata": {
    "language": "python",
    "name": "SnowflakeMLExplainability"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Step 1 ‚Äî Run the EXPLAIN function on a sample input\nmodel_version = registry.get_model(\"COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\").version(\"V2\")\nexplanations_df = model_version.run(\n    X_train.sample(n=50, random_state=42),\n    function_name=\"EXPLAIN\"  # --> Use Snowflake's new EXPLAIN function\n)\n\n# Step 2 ‚Äî Identify explanation columns (excluding OHE columns)\nexplanation_cols = [\n    col for col in explanations_df.columns\n    if col.endswith(\"_explanation\") and \"_OHE_\" not in col\n]\n\n# Step 3 ‚Äî Horizontal Bar Plot (Snowflake Explain)\nmean_abs_explanations = explanations_df[explanation_cols].abs().mean().sort_values(ascending=True)\n\nplt.figure(figsize=(4, 2))\nmean_abs_explanations.plot(kind=\"barh\")\nplt.title(\"SHAP-style Bar Plot (Snowflake EXPLAIN)\")\nplt.xlabel(\"Mean |Contribution|\")\nplt.ylabel(\"Feature\")\nplt.show()\n\n# Step 4 ‚Äî Beeswarm-style Dot Plot\ndot_data = explanations_df[explanation_cols].copy()\ndot_data.columns = [col.replace(\"_explanation\", \"\") for col in dot_data.columns]\ndot_data = dot_data.melt(var_name=\"Feature\", value_name=\"Contribution\")\n\nplt.figure(figsize=(4, 2))\nsns.stripplot(\n    data=dot_data,\n    x=\"Contribution\",\n    y=\"Feature\",\n    size=4,\n    alpha=0.6,\n    jitter=True,\n    orient=\"h\"\n)\nplt.title(\"Beeswarm-style Dot Plot (Snowflake EXPLAIN)\")\nplt.xlabel(\"Contribution Value\")\nplt.ylabel(\"Feature\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "f2939d58-3d26-4295-b185-6457345af275",
   "metadata": {
    "collapsed": false,
    "name": "SHAPSummary"
   },
   "source": [
    "### üìä Explainability Output (Snowflake `EXPLAIN` Function)\n",
    "\n",
    "These charts show the **contribution of numeric input features** (excluding one-hot-encoded columns) as derived using Snowflake‚Äôs built-in `EXPLAIN` capability.\n",
    "\n",
    "### üîµ Top Chart: Mean Absolute Contribution (Bar Plot)\n",
    "- Shows the **average absolute contribution** for each numeric feature across 50 examples.\n",
    "- Longer bars indicate **greater overall influence** on the model‚Äôs prediction.\n",
    "- In this case, `LOAN_AMOUNT_000S` has the largest impact, followed by `APPLICANT_INCOME_000S`.\n",
    "\n",
    "### üî¥ Bottom Chart: Dot Plot (Beeswarm-style)\n",
    "- Displays each example‚Äôs **raw contribution value** on the x-axis.\n",
    "- The **vertical spread** shows variability across inputs; cluster density indicates concentration of effects.\n",
    "- Useful to visualize how certain features push predictions **positively or negatively**.\n",
    "\n",
    "Together, these plots improve transparency by surfacing **how much and in which direction** the input features are influencing predictions in your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a8e84-953b-4ac2-af10-87113eeba74e",
   "metadata": {
    "collapsed": false,
    "name": "MLOpsModelPromotionViaAliases"
   },
   "source": [
    "### ‚úÖ Promote and Use Production Model with Aliases\n",
    "Now that we‚Äôve registered multiple versions of our model, we‚Äôll adopt a lifecycle strategy using aliases ‚Äî a flexible and production-ready approach supported by the **Snowflake Model Registry**.\n",
    "\n",
    "Instead of hardcoding version numbers (e.g., V1, V2), we assign an alias like production to the latest validated model version. This enables us to:\n",
    "- üîÑ Seamlessly promote new versions without updating downstream code\n",
    "- üîí Maintain cleaner, version-agnostic pipeline logic\n",
    "- ‚öôÔ∏è Support rollback or staged rollout by shifting the alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60830013-75cc-4ef6-97be-af5cd46765de",
   "metadata": {
    "language": "sql",
    "name": "DecommissionV1"
   },
   "outputs": [],
   "source": "-- Assign alias 'DECOMMISSIONED' to V1 for tracking purposes\nALTER MODEL COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL VERSION V1 SET ALIAS = DECOMMISSIONED;   -- Use Snowflake's ALIAS model parameter\n\n-- Assign alias 'PRODUCTION' to V2\nALTER MODEL COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL VERSION V2 SET ALIAS = PRODUCTION;       -- Set V2 as alias = PRODUCTION"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532121b-fb19-411c-9f64-c804af949601",
   "metadata": {
    "language": "sql",
    "name": "DORA_SEAI54"
   },
   "outputs": [],
   "source": [
    "-- SEAI54: Validate that Version V2 has the PRODUCTION alias\n",
    "SELECT util_db.public.se_grader(\n",
    "  'SEAI54',\n",
    "  (actual >= 1),\n",
    "  actual,\n",
    "  1,\n",
    "  '‚úÖ Alias \"PRODUCTION\" correctly assigned to Version V2!'\n",
    ") AS graded_results\n",
    "FROM (\n",
    "  SELECT COUNT(*) AS actual\n",
    "  FROM AICOLLEGE.INFORMATION_SCHEMA.MODEL_VERSIONS\n",
    "  WHERE MODEL_NAME = 'COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL'\n",
    "    AND MODEL_VERSION_NAME = 'V2'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43180de9-bfa9-47e3-8d92-fdde46a512c6",
   "metadata": {
    "collapsed": false,
    "name": "ModelRollBack"
   },
   "source": [
    "### üîÑ Roll Back Production Alias to V1\n",
    "\n",
    "If you ever need to roll back your `production` alias from V2 back to V1, run these steps **after** your example pipelines:\n",
    "\n",
    "```sql\n",
    "-- 1) Remove the PRODUCTION alias from V2\n",
    "ALTER MODEL AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\n",
    "  VERSION V2\n",
    "  UNSET ALIAS;\n",
    "\n",
    "-- 2) Clear any DECOMMISSIONED or other alias on V1\n",
    "ALTER MODEL AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\n",
    "  VERSION V1\n",
    "  UNSET ALIAS;\n",
    "\n",
    "-- 3) Assign the PRODUCTION alias back to V1\n",
    "ALTER MODEL AICOLLEGE.PUBLIC.COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\n",
    "  VERSION V1\n",
    "  SET ALIAS = PRODUCTION;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edffba-4a5a-4222-ab8d-31762f2084e3",
   "metadata": {
    "collapsed": false,
    "name": "ScorePromotedModelExamples"
   },
   "source": [
    "### Score with the Promoted Production Model\n",
    "\n",
    "With multiple model versions registered, we now use the **PRODUCTION** alias to serve both class labels and probability scores‚Äîwithout ever hard-coding a version number.\n",
    "\n",
    "This example shows how to:\n",
    "\n",
    "- Pull recent rows from the cleaned & encoded training data (`NEWTRAININGDATA_FINAL`)\n",
    "- Join with the Feature Store (`LOAN_FEATURES_OHE$1`)\n",
    "- Dynamically score new data using the latest production model, capturing:\n",
    "  - üî¢ **SCORE**: the model‚Äôs probability that the positive class occurs  \n",
    "  - üî¢ **PREDICTION**: the hard 0/1 label\n",
    "- Persist a single table with both fields for downstream reporting or monitoring\n",
    "\n",
    "By binding downstream code to the **PRODUCTION** alias you get:\n",
    "\n",
    "- üîÑ **Version-agnostic pipelines** ‚Äî no edits when you retrain or promote  \n",
    "- üîê **Instant rollback** ‚Äî simply retarget the alias if you discover an issue  \n",
    "- ‚öôÔ∏è **Plug-and-play automation** (Tasks, dbt, dynamic tables, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b24b1-a69d-4d03-8f37-f67058fa651a",
   "metadata": {
    "language": "python",
    "name": "ExampleBatchScoring"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# --- Initialize Registry and Get Model Version ---\n",
    "reg = Registry(session=session, database_name=\"AICOLLEGE\", schema_name=\"PUBLIC\")\n",
    "model_prod = reg.get_model(\"COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL\").version(\"production\")\n",
    "\n",
    "# --- Load Feature Store Data ---\n",
    "df_features = session.table(\"XXX\")\n",
    "\n",
    "# --- Drop rows with NULLs before inference ---\n",
    "df_features_clean = df_features.dropna()\n",
    "\n",
    "# --- Run prediction using PREDICT_PROBA ---\n",
    "df_scores = model_prod.run(df_features_clean, function_name=\"PREDICT_PROBA\")\n",
    "\n",
    "# --- Safely check output feature names ---\n",
    "print(\"Returned columns:\", df_scores.columns)\n",
    "\n",
    "# --- Build final prediction DataFrame ---\n",
    "df_final = (\n",
    "    df_scores\n",
    "    .with_column(\"PROBABILITY_SCORE\", col('\"output_feature_1\"'))  # use double quotes to handle Snowflake case sensitivity\n",
    "    .with_column(\"PREDICTION_LABEL\", (col('\"output_feature_1\"') > 0.5).cast(\"INT\"))\n",
    ")\n",
    "\n",
    "# --- Save predictions ---\n",
    "df_final.write.mode(\"overwrite\").save_as_table(\"AICOLLEGE.PUBLIC.MORTGAGE_PREDICTIONS_PROD\")\n",
    "\n",
    "print(\"‚úÖ Scoring pipeline completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2deea5-05aa-4378-ad14-f455c755ff0d",
   "metadata": {
    "name": "HOLFeedback"
   },
   "source": [
    "### üß† Feedback is Fuel  \n",
    "We‚Äôd love your input to help improve this lab! Please take 3 minutes to fill out our feedback form:  \n",
    "üëâ [Submit Feedback Here](https://docs.google.com/forms/d/e/1FAIpQLSdJwMBSnYffjJI5N9db3IjHz36dMtcpvI1YmQuMwO0EgAUS8A/viewform?usp=header)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdff86-5d66-4cc6-a565-3a0c18c2d3cb",
   "metadata": {
    "collapsed": false,
    "name": "HOLCleanup"
   },
   "source": [
    "### üßπ HOL Cleanup\n",
    "\n",
    "To reset your environment and avoid lingering monitors or scheduled alerts, you can drop the model monitor and alert below.\n",
    "\n",
    "This is especially useful when:\n",
    "\n",
    "- You're sharing an environment with other users\n",
    "- You plan to rerun the notebook from the top\n",
    "- You want to avoid refresh or alert-triggering compute\n",
    "\n",
    "‚ö†Ô∏è Skip this section if you're actively monitoring your model in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6cbbf-80fa-48a8-abd3-78df4df00773",
   "metadata": {
    "language": "sql",
    "name": "Cleanup"
   },
   "outputs": [],
   "source": [
    "-- HOL CLEANUP SCRIPT\n",
    "\n",
    "-- Drop model monitors first (they may reference the model)\n",
    "DROP MODEL MONITOR IF EXISTS MORTGAGE_MODEL_MONITOR_V1;\n",
    "DROP MODEL MONITOR IF EXISTS MORTGAGE_MODEL_MONITOR_V2;\n",
    "\n",
    "-- -- Drop the ML model\n",
    "DROP MODEL IF EXISTS COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL;\n",
    "\n",
    "-- Drop dynamic feature table (feature store)\n",
    "DROP DYNAMIC TABLE IF EXISTS LOAN_FEATURES_OHE$1;\n",
    "\n",
    "-- Drop the Feature View\n",
    "DROP FEATURE VIEW AICOLLEGE.PUBLIC.LOAN_FEATURES_OHE VERSION 1;\n",
    "\n",
    "-- Drop the Entity\n",
    "DROP ENTITY AICOLLEGE.PUBLIC.LOAN_ENTITY;\n",
    "\n",
    "-- Drop intermediate and prediction tables\n",
    "DROP TABLE IF EXISTS ALL_PREDICTIONS_WITH_GROUND_TRUTH;\n",
    "DROP TABLE IF EXISTS BASELINE_PREDICTIONS;\n",
    "DROP TABLE IF EXISTS INFERENCEMORTGAGEDATA;\n",
    "DROP TABLE IF EXISTS MORTGAGE_PREDICTIONS_PROD;\n",
    "DROP TABLE IF EXISTS NEWTRAININGDATA;\n",
    "DROP TABLE IF EXISTS NEWTRAININGDATA_CLEANED;\n",
    "DROP TABLE IF EXISTS NEWTRAININGDATA_FINAL;\n",
    "DROP TABLE IF EXISTS PREDICTIONS_WITH_GROUND_TRUTH;\n",
    "DROP VIEW AICOLLEGE.PUBLIC.V2_PREDICTIONS;\n",
    "DROP TABLE AICOLLEGE.PUBLIC.V2_RAW_PREDICTIONS;\n",
    "\n",
    "-- Drop any custom file formats used (adjust name if needed)\n",
    "DROP FILE FORMAT IF EXISTS MLOPS;"
   ]
  }
 ]
}