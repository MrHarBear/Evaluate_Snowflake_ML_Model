{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "htih7yloj6yr7opyo2ez",
   "authorId": "167081822753",
   "authorName": "HCHEN",
   "authorEmail": "harley.chen@snowflake.com",
   "sessionId": "229b5ae0-7a81-4a25-b334-6cd281aeac99",
   "lastEditTime": 1751906766529
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "MLOps_ML_Observability"
   },
   "outputs": [],
   "source": "# Import Snowpark context for the active session\nimport streamlit as st\nimport pandas as pd\nimport altair as alt\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Define image in a stage and read the file\n#image=session.file.get_stream('@aicollege.public.setup/MLObservabilityWorkflow.jpg' , decompress=False).read() \n\n# Display the image\n#st.image(image, width=800)"
  },
  {
   "cell_type": "markdown",
   "id": "21ef590c-782f-4a38-80cb-c2532ae4e710",
   "metadata": {
    "collapsed": false,
    "name": "MLObservabilityFlow"
   },
   "source": [
    "## 📊 ML Observability in Action: Monitoring + Alerting Workflow\n",
    "\n",
    "In this section, you’ll track and respond to model performance using **Snowflake Model Monitors**. Here's the recommended end-to-end workflow:\n",
    "\n",
    "1. Create a **baseline table** using early batch inference data (e.g., Weeks 1–5).\n",
    "2. Set up a **model monitor** that references the baseline to enable drift metrics immediately.\n",
    "3. Explore key **performance metrics** in **Snowsight**, such as **AUC**, **F1 Score**, **Precision**, and **Recall**.\n",
    "4. Enable all **8 Snowsight panels** (prediction count, actuals count, performance, drift, etc.) for full visibility.\n",
    "5. Create an **alert** that triggers if **F1 Score** drops below a specified threshold (e.g., `0.7`).\n",
    "6. Recreate the model monitor using **all available inference data**, while continuing to reference the baseline.\n",
    "7. Understand when **drift metrics may become disabled** and how to **re-enable them** (e.g., when baseline is missing).\n",
    "8. Use functions like `MODEL_MONITOR_PERFORMANCE_METRIC` and `MODEL_MONITOR_DRIFT_METRIC` to **query metrics over time**.\n",
    "9. **Visualize trends** in Streamlit or matplotlib using custom dashboards — including manual drift analysis via **Hellinger Distance**.\n",
    "\n",
    "This notebook walks you through each step to build a transparent, production-ready observability pipeline using the Snowflake Model Registry and Monitoring framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d7e09-3f99-46c6-86a9-9ddd6d6dbb17",
   "metadata": {
    "collapsed": false,
    "name": "BaselineTable"
   },
   "source": [
    "### 📌 Create a Baseline Table for Drift Monitoring\n",
    "\n",
    "To enable drift metrics in Snowflake Model Monitoring, we must define a **baseline dataset**. This baseline acts as the reference point for detecting data drift over time.\n",
    "\n",
    "In this lab, we’ll use the **first five weeks** of batch inference results as our baseline. These predictions represent a stable period before performance begins to shift.\n",
    "\n",
    "The baseline will be stored in a table called `BASELINE_PREDICTIONS`. We’ll use it when creating our model monitor so that drift metrics are enabled from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc06d91-630c-4e31-a2ef-6d1e3e2204e5",
   "metadata": {
    "language": "sql",
    "name": "CreateBaselineTable"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE AICOLLEGE.PUBLIC.BASELINE_PREDICTIONS AS  --- Create baseline_predictions table\nSELECT *\nFROM AICOLLEGE.PUBLIC.PREDICTIONS_WITH_GROUND_TRUTH  --- Use Week-by-week (Weeks 1–5) PREDICTIONS_WITH_GROUND_TRUTH as input\nWHERE WEEK BETWEEN 1 AND 5;"
  },
  {
   "cell_type": "code",
   "id": "d1229045-3dd9-4130-b52d-8820885b3112",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "SELECT *\nFROM AICOLLEGE.PUBLIC.PREDICTIONS_WITH_GROUND_TRUTH",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0961710a-b396-4977-a3ee-790d2b4dccbf",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "DESC TABLE AICOLLEGE.PUBLIC.PREDICTIONS_WITH_GROUND_TRUTH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e61fedd-ada7-4131-95fd-68b075be8c8e",
   "metadata": {
    "collapsed": false,
    "name": "CreateModelMonitor"
   },
   "source": [
    "### 🛠️ Set Up the Model Monitor\n",
    "\n",
    "Now that we’ve completed batch inference and saved the results, it’s time to enable ML Observability by creating a **Model Monitor**.\n",
    "\n",
    "Snowflake model monitors allow you to:\n",
    "\n",
    "- Track your model’s **performance** over time (accuracy, precision, recall)\n",
    "- Detect **data drift** or changes in prediction confidence\n",
    "- Monitor **prediction volume** and row-level trends\n",
    "\n",
    "Each model monitor must be tied to:\n",
    "- A specific model version\n",
    "- A source table with inference + actuals\n",
    "- A timestamp column for time-based aggregation\n",
    "\n",
    "We’ll now use the `CREATE MODEL MONITOR` command to register our monitor and start collecting observability metrics. Once the monitor is active, you’ll be able to view insights in the **Snowsight Monitoring Dashboard** under `AI & ML > Models`.\n",
    "\n",
    "> ⚠️ Make sure your source table includes the following:\n",
    "> - A `PREDICTED_RESPONSE` column (binary)\n",
    "> - A `PREDICTED_SCORE` column (probability between 0 and 1)\n",
    "> - A `MORTGAGERESPONSE` column (ground truth)\n",
    "> - A `WEEK_DATE` or timestamp column (must be type `DATE` or `TIMESTAMP_NTZ`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ac45f-13fd-41c2-9624-aec73b54b180",
   "metadata": {
    "language": "sql",
    "name": "CreateMORTGAGE_MODEL_MONITOR"
   },
   "outputs": [],
   "source": "use role aicollege;\nCREATE OR REPLACE MODEL MONITOR MORTGAGE_MODEL_MONITOR  --- Create mortgage model monitor\nWITH \n  MODEL = 'COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL',  -- Use registered model name\n  VERSION = 'V1',  -- Registered version\n  FUNCTION = 'predict',  --- Use predict function\n  \n  -- Full inference table containing predictions + ground truth\n  SOURCE = AICOLLEGE.PUBLIC.PREDICTIONS_WITH_GROUND_TRUTH,  --- Use Week-by-week (Weeks 1–5) PREDICTIONS_WITH_GROUND_TRUTH as initial input\n  \n  -- Enables drift metrics using early scoring results (Weeks 1–5)\n  BASELINE = AICOLLEGE.PUBLIC.BASELINE_PREDICTIONS,  --- Use baseline table\n  \n  WAREHOUSE = AICOLLEGE,\n  REFRESH_INTERVAL = '365 DAY',\n  AGGREGATION_WINDOW = '7 DAYS',\n  \n  -- Timestamp must be TIMESTAMP_NTZ\n  TIMESTAMP_COLUMN = WEEK_START_DATE,  \n  \n  ID_COLUMNS = ('LOAN_ID'),  -- Row identifier\n  PREDICTION_CLASS_COLUMNS = ('PREDICTED_RESPONSE'),  --- Use binary prediction\n  ACTUAL_CLASS_COLUMNS = ('MORTGAGERESPONSE'),  --- Ground truth\n  PREDICTION_SCORE_COLUMNS = ('PREDICTED_SCORE');  --- Use model confidence score"
  },
  {
   "cell_type": "markdown",
   "id": "c505f414-a07f-41f6-8b69-50b8b34dcdfa",
   "metadata": {
    "collapsed": false,
    "name": "ReviewModelMonitorMetrics"
   },
   "source": [
    "### ✅ View the Monitor in Snowsight\n",
    "\n",
    "To access your model's observability dashboard:\n",
    "\n",
    "1. In Snowsight, go to **AI & ML > Models**.\n",
    "2. Locate your model: **COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL**.\n",
    "3. Click into the model, then scroll down to the **Monitors** section.\n",
    "4. Click on your monitor: **MORTGAGE_MODEL_MONITOR**.\n",
    "\n",
    "You’ll now see a dashboard that includes:\n",
    "\n",
    "- 📈 **Performance metrics** – Tracks accuracy, precision, and recall over time  \n",
    "- 🧪 **Drift detection** – Highlights shifts in data distribution using metrics like Hellinger distance  \n",
    "- 📊 **Volume stats** – Shows trends in row counts and prediction volume  \n",
    "- 🕒 Use the **time range selector** at the top to explore different time windows.\n",
    "\n",
    "📌 If your predictions were generated weekly, the **WEEK_START_DATE** column will anchor the time-based aggregation used in the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252348e9-f2ea-49b2-8e3b-031bc119d31f",
   "metadata": {
    "collapsed": false,
    "name": "CreateAlert"
   },
   "source": [
    "### 🔔 Set Up an `F1_SCORE` Alert for Model Monitoring\n",
    "\n",
    "To automate performance monitoring, we'll create a **Snowflake alert** that checks if the model’s `F1_SCORE` drops below `0.7`.\n",
    "\n",
    "📉 **Why 0.7?**  \n",
    "An **F1 score below 0.7** often indicates a **significant degradation in model performance**. This threshold reflects a tradeoff between **precision (false positives)** and **recall (false negatives)** — and a dip below this level may signal:\n",
    "- Poor model generalization to new data\n",
    "- Increased number of incorrect classifications\n",
    "- Potential data quality issues or concept drift\n",
    "\n",
    "If this happens, the alert will trigger an **email notification** to the designated recipient — allowing MLOps teams to take proactive action when model performance drops below acceptable levels.\n",
    "\n",
    "---\n",
    "\n",
    "**🧪 Testing the Alert**\n",
    "\n",
    "For demonstration purposes, we’ll use a **fixed date range** around **mid-February 2025**, when we know the `F1_SCORE` dips below the threshold.  \n",
    "This ensures the alert fires immediately — no matter when the notebook is run.\n",
    "\n",
    "Later, this can be swapped for a rolling time window (e.g., `CURRENT_DATE() - 7`) for ongoing monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "**🔍 What this alert does:**\n",
    "- Queries the `MODEL_MONITOR_PERFORMANCE_METRIC` function for recent `F1_SCORE` values  \n",
    "- Checks values between **Feb 9 and Feb 16**\n",
    "- Triggers an email via the `ML_ALERTS` integration if any value falls below `0.7`\n",
    "- Runs daily on a schedule (e.g., 9 AM UTC)\n",
    "\n",
    "Once verified, this logic can be adapted for long-term production monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd9630-ac85-456e-b26c-6703e9cc9478",
   "metadata": {
    "language": "sql",
    "name": "SetAlert"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE ALERT F1_SCORE_DROP_ALERT  -- Set your model monitor alert called \"F1_SCORE_DROP_ALERT\"\n  WAREHOUSE = AICOLLEGE\n  SCHEDULE = 'USING CRON 0 9 * * * UTC'\n  IF (EXISTS (\n    SELECT 1\n    FROM TABLE(\n      MODEL_MONITOR_PERFORMANCE_METRIC(\n        'MORTGAGE_MODEL_MONITOR',\n        'F1_SCORE',\n        '7 DAYS',\n        '2025-02-09'::DATE,  -- Fixed start date for simulation\n        '2025-02-16'::DATE   -- Fixed end date for simulation\n        -- DATEADD(DAY, -7, CURRENT_DATE()), CURRENT_DATE() -- Use this for production later\n      )\n    )\n    WHERE METRIC_VALUE < 0.7 -- Set your F1 score threshold to 0.7\n  ))\n  THEN CALL SYSTEM$SEND_EMAIL(\n    'ML_ALERTS',\n    'harley.chen@snowflake.com',  --- Update with your email\n    'F1_SCORE dropped below threshold',\n    'The F1_SCORE for COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL has dropped below 0.7 between Feb 9 and Feb 16.'\n  );"
  },
  {
   "cell_type": "markdown",
   "id": "76604708-8a53-47a8-b0f5-5267c3674585",
   "metadata": {
    "collapsed": false,
    "name": "RecreateModelMonitor"
   },
   "source": [
    "### 🔁 Recreate the Model Monitor with Full Historical Data\n",
    "\n",
    "Now that we've added more predictions to the `ALL_PREDICTIONS_WITH_GROUND_TRUTH` table (beyond just Weeks 1–5), it's important to **recreate the model monitor** to include all available inference results.\n",
    "\n",
    "Because a model monitor can only be tied to a single source table and model version, the best practice is to:\n",
    "\n",
    "1. **Drop the existing monitor** that only covered early batch inference.\n",
    "2. **Recreate the monitor** using the same model and version — but now referencing the full table of predictions.\n",
    "\n",
    "This ensures your observability dashboards and performance metrics in Snowsight reflect a more complete and up-to-date view of the model’s behavior in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0b69f-e8d6-4714-92c8-9c8a0a150526",
   "metadata": {
    "language": "sql",
    "name": "UpdateModelMonitor"
   },
   "outputs": [],
   "source": "use role aicollege;\n-- Drop the previous monitor to refresh with all historical data\n-- DROP MODEL MONITOR IF EXISTS MORTGAGE_MODEL_MONITOR;\n\n-- Recreate the monitor using the full prediction table and a defined baseline\nCREATE MODEL MONITOR MORTGAGE_MODEL_MONITOR\nWITH \n  MODEL = 'COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL',\n  VERSION = 'V1',\n  FUNCTION = 'predict',\n  SOURCE = AICOLLEGE.PUBLIC.PREDICTIONS_WITH_GROUND_TRUTH,  --- Use full table batch scoring Snowflake table\n  BASELINE = AICOLLEGE.PUBLIC.BASELINE_PREDICTIONS,\n  WAREHOUSE = AICOLLEGE,\n  REFRESH_INTERVAL = '365 DAY',\n  AGGREGATION_WINDOW = '7 DAYS',\n  TIMESTAMP_COLUMN = WEEK_START_DATE,\n  ID_COLUMNS = ('LOAN_ID'),  -- Row identifier\n  PREDICTION_CLASS_COLUMNS = ('PREDICTED_RESPONSE'),  --- Use binary prediction\n  ACTUAL_CLASS_COLUMNS = ('MORTGAGERESPONSE'),  --- Ground truth\n  PREDICTION_SCORE_COLUMNS = ('PREDICTED_SCORE');  --- Use model confidence score"
  },
  {
   "cell_type": "markdown",
   "id": "dd42e34f-c6a8-4dcb-9a1d-ba167477adb6",
   "metadata": {
    "collapsed": false,
    "name": "TriggerImmediateAlertTesting"
   },
   "source": [
    "### 📣 Trigger the Alert Immediately for Testing\n",
    "\n",
    "After defining an alert to monitor model performance, Snowflake will only evaluate it based on the scheduled time (e.g., daily at 9 AM UTC).\n",
    "\n",
    "To test your alert setup immediately — and confirm that the logic and email integration work as expected — you can:\n",
    "\n",
    "- **Resume the alert** (if it’s currently paused)\n",
    "- **Manually execute the alert** to trigger the condition check\n",
    "\n",
    "This is useful for:\n",
    "\n",
    "- Verifying that your alert logic behaves as expected  \n",
    "- Confirming that email notifications are successfully sent  \n",
    "- Validating the threshold you’ve set for model metrics  \n",
    "\n",
    "> ⚠️ The alert will continue to run automatically based on its schedule unless you pause or drop it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef6a9c-1516-4179-9c17-04042a624cc3",
   "metadata": {
    "language": "sql",
    "name": "GenerateAlert"
   },
   "outputs": [],
   "source": [
    "-- ✅ Resume the alert if it was paused\n",
    "ALTER ALERT F1_SCORE_DROP_ALERT RESUME;\n",
    "\n",
    "-- 🚀 Execute the alert immediately to check if an email should be sent\n",
    "EXECUTE ALERT F1_SCORE_DROP_ALERT;\n",
    "\n",
    "-- 📬 After executing the alert, check your email inbox (or other notification channel)\n",
    "-- to confirm that the alert was triggered and delivered successfully.\n",
    "-- Make sure your alert notification settings are correctly configured in Snowflake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485245a4-56d0-48af-bec9-9d5956e785ad",
   "metadata": {
    "collapsed": false,
    "name": "RecheckModelMonitorResults"
   },
   "source": [
    "### 🔁 Review Updated Monitor Results in Snowsight\n",
    "\n",
    "With the full prediction history now included in the monitor, revisit Snowsight to explore updated trends.\n",
    "\n",
    "1. Go to **AI & ML > Models**, then locate:\n",
    "   `COLLEGE_AI_HOL_XGB_MORTGAGE_MODEL > V1 > MORTGAGE_MODEL_MONITOR`\n",
    "\n",
    "2. Use **Settings** to enable:\n",
    "   - AUC, F1 Score, Precision, Recall, Classification Accuracy\n",
    "   - Drift metrics: Prediction + Feature value drift (now available since a **baseline** was provided)\n",
    "\n",
    "3. For drift metrics, select features like:\n",
    "   - **PREDICTED_RESPONSE**, **PREDICTED_SCORE** for prediction drift\n",
    "   - Input features (e.g., INCOME, LOAN_TYPE_NAME) for feature drift\n",
    "\n",
    "This visibility helps track degradation over time and detect potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde2dbb-5a49-4a38-89b1-956196513035",
   "metadata": {
    "collapsed": false,
    "name": "ProgrammaticMLMonitoring"
   },
   "source": [
    "## 📊 ML Monitoring: Custom Drift & Performance Charts in Streamlit\n",
    "\n",
    "While **Snowsight provides a rich UI** for exploring model performance and drift, you may want to build a **custom dashboard using Streamlit** — especially for embedding within existing **MLOps workflows** or sharing **lightweight visualizations**.\n",
    "\n",
    "This section shows how to:\n",
    "\n",
    "- **Programmatically access performance, drift, and volume metrics**\n",
    "- **Create time-series visualizations** using metrics like **AUC**, **F1 Score**, **Precision**, and **Recall**\n",
    "- **Monitor distribution drift** using metrics like **Jensen-Shannon distance** or **Hellinger distance**\n",
    "- **Visualize predictions vs actuals over time** using interactive charts\n",
    "\n",
    "This approach works well when you have a **registered model with an attached monitor** (as shown in Snowsight), and want to **surface select insights** for stakeholders or dev teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79d727-b059-4461-804a-1f037a2101ff",
   "metadata": {
    "language": "python",
    "name": "SimpleAUCvsTime"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport streamlit as st\n\n# Query ROC_AUC from model monitor using updated column names\nquery = \"\"\"\n-- Use Snowflake ML MODEL_MONITOR_PERFORMANCE_METRIC function to retrieve ROC AUC over time\nSELECT\n  EVENT_TIMESTAMP,\n  METRIC_VALUE AS ROC_AUC\nFROM TABLE(\n    MODEL_MONITOR_PERFORMANCE_METRIC(   #  -->  Use Snowflake's MODEL_MONITOR_PERFORMANCE_METRIC\n    'MORTGAGE_MODEL_MONITOR',\n    'ROC_AUC',\n    '7 DAYS',\n    DATEADD(DAY, -90, CURRENT_DATE()),\n    CURRENT_DATE()\n  )\n)\nORDER BY EVENT_TIMESTAMP\n\"\"\"\n\n# Run the query\ndf_auc = session.sql(query).to_pandas()\n\n# Plotting the AUC over time\nplt.figure(figsize=(10, 4))\nplt.plot(df_auc[\"EVENT_TIMESTAMP\"], df_auc[\"ROC_AUC\"], marker='o', linestyle='-')\nplt.title(\"AUC Over Time\")\nplt.xlabel(\"Event Timestamp\")\nplt.ylabel(\"ROC AUC\")\nplt.xticks(rotation=45)\nplt.grid(True)"
  },
  {
   "cell_type": "markdown",
   "id": "ea196477-1d37-4c1a-9c29-e59c9a1cec7a",
   "metadata": {
    "collapsed": false,
    "name": "WeeklySummaryViewData"
   },
   "source": [
    "### 🧮 Create Weekly Summary View for Feature Tracking\n",
    "\n",
    "To visualize sampled feature trends across weeks, we create a lightweight view with one sample per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583ae95-dbbc-4f1d-a458-771b2b25ccf6",
   "metadata": {
    "language": "sql",
    "name": "AggregateWeeklyMetrics"
   },
   "outputs": [],
   "source": [
    "-- Create a table/view with one sample row per week to track prediction trends\n",
    "CREATE OR REPLACE VIEW AICOLLEGE.PUBLIC.WEEKLY_MONITOR_SAMPLE AS\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY WEEK_START_DATE ORDER BY LOAN_ID) AS row_num\n",
    "    FROM AICOLLEGE.PUBLIC.ALL_PREDICTIONS_WITH_GROUND_TRUTH\n",
    ")\n",
    "WHERE row_num = 1\n",
    "ORDER BY WEEK_START_DATE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed8df2-78b0-4781-a0e8-cbc9de31ee66",
   "metadata": {
    "language": "python",
    "name": "StreamlitMLMonitoringCode"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "monitor_name = \"MORTGAGE_MODEL_MONITOR\"\n",
    "\n",
    "tab1, tab2 = st.tabs([\"📈 Model Metrics\", \"🔍 Weekly Feature Trends\"])\n",
    "\n",
    "# TAB 1 — MODEL METRICS\n",
    "with tab1:\n",
    "    st.subheader(\"📊 Model Performance Over Time\")\n",
    "\n",
    "    metrics = {\n",
    "        \"AUC\": \"ROC_AUC\",\n",
    "        \"F1 Score\": \"F1_SCORE\",\n",
    "        \"Accuracy\": \"CLASSIFICATION_ACCURACY\",\n",
    "        \"Precision\": \"PRECISION\",\n",
    "        \"Recall\": \"RECALL\"\n",
    "    }\n",
    "\n",
    "    selected_metric_label = st.selectbox(\"📈 Select a performance metric:\", list(metrics.keys()))\n",
    "    selected_metric = metrics[selected_metric_label]\n",
    "\n",
    "    metric_query = f\"\"\"\n",
    "        SELECT EVENT_TIMESTAMP, METRIC_VALUE\n",
    "        FROM TABLE(\n",
    "            MODEL_MONITOR_PERFORMANCE_METRIC(\n",
    "                '{monitor_name}',\n",
    "                '{selected_metric}',\n",
    "                '1 WEEK',\n",
    "                DATE '2025-01-01',\n",
    "                CURRENT_DATE()\n",
    "            )\n",
    "        )\n",
    "        ORDER BY EVENT_TIMESTAMP\n",
    "    \"\"\"\n",
    "\n",
    "    df = session.sql(metric_query).to_pandas()\n",
    "\n",
    "    if not df.empty:\n",
    "        st.markdown(f\"### 🧭 Metric Trend: **{selected_metric_label}**\")\n",
    "        line = alt.Chart(df).mark_line(point=True).encode(\n",
    "            x=alt.X(\"EVENT_TIMESTAMP:T\", title=\"Date\", axis=alt.Axis(format=\"%b %d\")),\n",
    "            y=alt.Y(\"METRIC_VALUE:Q\", title=selected_metric_label),\n",
    "            tooltip=[\"EVENT_TIMESTAMP\", \"METRIC_VALUE\"]\n",
    "        ).properties(width=700, height=400)\n",
    "\n",
    "        st.altair_chart(line, use_container_width=True)\n",
    "        with st.expander(\"📋 Show raw metric values\"):\n",
    "            st.dataframe(df)\n",
    "    else:\n",
    "        st.warning(\"⚠️ No data found for the selected metric.\")\n",
    "\n",
    "# TAB 2 — RAW WEEKLY FEATURE TRENDS\n",
    "with tab2:\n",
    "    st.subheader(\"🔍 Weekly Sampled Feature Trends\")\n",
    "\n",
    "    feature_options = [\n",
    "        \"MORTGAGERESPONSE\", \n",
    "        \"PREDICTED_RESPONSE\", \n",
    "        \"PREDICTED_SCORE\",\n",
    "        \"APPLICANT_INCOME_000S\",\n",
    "        \"LOAN_AMOUNT_000S\"\n",
    "    ]\n",
    "\n",
    "    selected_feature = st.selectbox(\"🧮 Select a feature to track weekly:\", feature_options)\n",
    "\n",
    "    trend_query = f\"\"\"\n",
    "        SELECT WEEK_START_DATE, {selected_feature}\n",
    "        FROM AICOLLEGE.PUBLIC.WEEKLY_MONITOR_SAMPLE\n",
    "        ORDER BY WEEK_START_DATE\n",
    "    \"\"\"\n",
    "\n",
    "    trend_df = session.sql(trend_query).to_pandas()\n",
    "\n",
    "    if not trend_df.empty:\n",
    "        st.markdown(f\"### 📊 Weekly Trend: **{selected_feature}** (sampled)\")\n",
    "        chart = alt.Chart(trend_df).mark_line(point=True).encode(\n",
    "            x=alt.X(\"WEEK_START_DATE:T\", title=\"Week Start\", axis=alt.Axis(format=\"%b %d\")),\n",
    "            y=alt.Y(f\"{selected_feature}:Q\", title=selected_feature),\n",
    "            tooltip=[\"WEEK_START_DATE\", selected_feature]\n",
    "        ).properties(width=700, height=400)\n",
    "\n",
    "        st.altair_chart(chart, use_container_width=True)\n",
    "\n",
    "        with st.expander(\"📋 Show raw weekly feature data\"):\n",
    "            st.dataframe(trend_df)\n",
    "    else:\n",
    "        st.warning(\"⚠️ No weekly data found for this feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f45e7-7ccd-40b6-96c9-b4c36f043f94",
   "metadata": {
    "collapsed": false,
    "name": "FeatureDrift"
   },
   "source": [
    "### 🔍 Feature Drift: Detecting Shifts in Input Data\n",
    "\n",
    "While **performance metrics** like **AUC** and **F1 Score** reveal how well a model is doing, it's equally important to monitor for **feature drift** — shifts in the distribution of input data over time.\n",
    "\n",
    "Even when performance appears stable, **drift in key input features** can signal:\n",
    "\n",
    "- **ETL or pipeline changes**\n",
    "- **Seasonal or regional trends**\n",
    "- **Concept drift** that may affect long-term accuracy\n",
    "\n",
    "Snowflake **ML Observability** provides built-in drift metrics like **Jensen-Shannon distance** and supports weekly comparisons. These metrics help detect early warning signs before they lead to degraded model performance.\n",
    "\n",
    "📌 In this example, we go further and simulate feature drift manually using **Hellinger Distance**, which compares two probability distributions:\n",
    "\n",
    "- A **distance of `0.0`** = identical distributions (**no drift**)\n",
    "- A **distance of `1.0`** = completely different distributions (**total drift**)\n",
    "\n",
    "This hands-on drift view complements Snowflake’s automated dashboards and enables **deeper exploratory analysis** using the raw inference data in `INFERENCEMORTGAGEDATA`.\n",
    "\n",
    "Use the dropdown below to:\n",
    "\n",
    "- Select a **numeric feature**\n",
    "- Choose **two different weeks**\n",
    "- Visualize how the distribution of that feature changes week-over-week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42503f66-c6b1-44c0-ab35-038210d6ad3d",
   "metadata": {
    "language": "python",
    "name": "VisualFeatureDrift"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "st.subheader(\"📐 Compare Feature Drift Using Hellinger Distance\")\n",
    "\n",
    "# Load the main prediction data with weekly timestamps\n",
    "df = session.table(\"AICOLLEGE.PUBLIC.ALL_PREDICTIONS_WITH_GROUND_TRUTH\").to_pandas()\n",
    "\n",
    "# Extract readable weekly labels\n",
    "df['WEEK'] = pd.to_datetime(df['WEEK_START_DATE']).dt.to_period('W').astype(str)\n",
    "weeks = sorted(df['WEEK'].unique())\n",
    "\n",
    "# Optional: display week labels as datetime strings for clearer dropdowns\n",
    "week_label_map = {\n",
    "    wk: f\"Week of {pd.Period(wk).end_time.strftime('%b %d, %Y')}\"\n",
    "    for wk in weeks\n",
    "}\n",
    "week_labels = list(week_label_map.values())\n",
    "\n",
    "# Dropdown selections\n",
    "numeric_features = [\n",
    "    \"APPLICANT_INCOME_000S\", \n",
    "    \"LOAN_AMOUNT_000S\", \n",
    "    \"PREDICTED_SCORE\"\n",
    "]\n",
    "feature = st.selectbox(\"🔢 Select a numeric feature:\", numeric_features)\n",
    "\n",
    "# Use dropdowns with friendly labels but map back to period keys\n",
    "selected_label1 = st.selectbox(\"📆 Select Week 1 (baseline):\", week_labels, index=0)\n",
    "selected_label2 = st.selectbox(\"📆 Select Week 2 (comparison):\", week_labels, index=1)\n",
    "week1 = [k for k, v in week_label_map.items() if v == selected_label1][0]\n",
    "week2 = [k for k, v in week_label_map.items() if v == selected_label2][0]\n",
    "\n",
    "# Filter the data\n",
    "dist1 = df[df['WEEK'] == week1][feature].dropna()\n",
    "dist2 = df[df['WEEK'] == week2][feature].dropna()\n",
    "\n",
    "# Compute Hellinger Distance\n",
    "def compute_hellinger(p, q):\n",
    "    return np.sqrt(0.5 * np.sum((np.sqrt(p) - np.sqrt(q)) ** 2))\n",
    "\n",
    "bins = np.histogram_bin_edges(np.concatenate([dist1, dist2]), bins=10)\n",
    "p_hist, _ = np.histogram(dist1, bins=bins, density=True)\n",
    "q_hist, _ = np.histogram(dist2, bins=bins, density=True)\n",
    "\n",
    "score = compute_hellinger(p_hist, q_hist)\n",
    "\n",
    "# Display interpretation tip\n",
    "st.info(\"ℹ️ A Hellinger Distance closer to 0.0 suggests low drift. Values near 1.0 indicate significant feature distribution changes.\")\n",
    "\n",
    "# Display result\n",
    "st.markdown(f\"### 📏 Hellinger Distance between **{selected_label1}** and **{selected_label2}** for **{feature}**: `{score:.4f}`\")\n",
    "\n",
    "# Plot histogram comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(dist1, bins=bins, alpha=0.6, label=selected_label1, color='blue')\n",
    "ax.hist(dist2, bins=bins, alpha=0.6, label=selected_label2, color='orange')\n",
    "ax.set_title(f\"Distribution Comparison: {feature}\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c086451-3859-4650-b07d-9722ee5b54a8",
   "metadata": {
    "language": "sql",
    "name": "DORA_SEAI52"
   },
   "outputs": [],
   "source": [
    "-- DORA Evaluation Test #52: Validate that the model monitor was created\n",
    "-- Step 1: Run SHOW command\n",
    "SHOW MODEL MONITORS;\n",
    "\n",
    "-- Step 2: Run this validation check\n",
    "SELECT util_db.public.se_grader(\n",
    "  'SEAI52',\n",
    "  (actual = expected),\n",
    "  actual,\n",
    "  expected,\n",
    "  '✅ Your MORTGAGE_MODEL_MONITOR has been successfully created!'\n",
    ") AS graded_results\n",
    "FROM (\n",
    "  SELECT COUNT(*) AS actual,\n",
    "         1 AS expected\n",
    "  FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n",
    "  WHERE \"name\" = 'MORTGAGE_MODEL_MONITOR'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506687aa-87de-4006-9b92-b1560d252377",
   "metadata": {
    "collapsed": false,
    "name": "HOLCleanup"
   },
   "source": [
    "### 🧹 HOL Cleanup\n",
    "\n",
    "To reset your environment and avoid lingering monitors or scheduled alerts, you can drop the model monitor and alert below.\n",
    "\n",
    "This is especially useful when:\n",
    "\n",
    "- You're sharing an environment with other users\n",
    "- You plan to rerun the notebook from the top\n",
    "- You want to avoid refresh or alert-triggering compute\n",
    "\n",
    "⚠️ Skip this section if you're actively monitoring your model in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb86f9f-5d98-4c20-a1cf-660cf488f80a",
   "metadata": {
    "language": "sql",
    "name": "HOLCleanupSQL"
   },
   "outputs": [],
   "source": [
    "-- HOL CLEANUP: Monitoring & Alerting Components\n",
    "\n",
    "-- Drop the alert first (it may reference the monitor or view)\n",
    "DROP ALERT IF EXISTS F1_SCORE_DROP_ALERT;\n",
    "\n",
    "-- Drop the model monitor (after removing dependent alert)\n",
    "DROP MODEL MONITOR IF EXISTS MORTGAGE_MODEL_MONITOR;\n",
    "\n",
    "-- Drop the monitoring view (if no longer needed)\n",
    "DROP VIEW IF EXISTS AICOLLEGE.PUBLIC.WEEKLY_MONITOR_SAMPLE;"
   ]
  }
 ]
}